{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistical Inference: IV\n",
    "\n",
    "*S. R. Taylor (2021)*\n",
    "\n",
    "Material in this lecture and notebook is based upon the lectures of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18). Also the \"Inference2\" lecture of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), Karen Leighly's [Bayesian Stats](http://seminar.ouml.org/lectures/bayesian-statistics/), and J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 5.\n",
    "- [MCMC Sampling](https://twiecki.io/blog/2015/11/10/mcmc-sampling) by Thomas Wiecki.\n",
    "- [Sampler, Samplers, Everywhere...](http://mattpitkin.github.io/samplers-demo/pages/samplers-samplers-everywhere/) by Matt Pitkin.\n",
    "- [MCMC Interactive Demo](https://chi-feng.github.io/mcmc-demo/app.html) by Chi Feng.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "* [Practical MCMC](#one)\n",
    "* [MCMC Parameter Estimation In The Wild](#two)\n",
    "* [MCMC With Emcee](#three)\n",
    "* [MCMC With PyMC3](#four)\n",
    "* [MCMC With The PTMCMCSampler](#five)\n",
    "\n",
    "---\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***In case you didn't do it last time:*** Please pause for a few minutes and install these packages `emcee`, `pymc3`, and `PTMCMCSampler` before going through today's notebook. Make sure this notebook is in the correct Python kernel for the class conda environment before executing each of the following cells in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/dfm/acor@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/jellis18/PTMCMCSampler@master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical MCMC <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from scipy import integrate\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy\n",
    "from astroML.plotting import hist\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical MCMC chain checks\n",
    "\n",
    "1. **CHECK ACCEPTANCE:** some MCMC samplers give an updating estimate of the current acceptance rate of new samples. Ideally for a sampler using some form of Metropolis-Hastings, this should be somewhere between $\\sim20-50\\%$ depending on the type of problem you're trying to solve.\n",
    "\n",
    "    - If the acceptance rate is high, the chain is moving but might not be exploring well. This gives high acceptance rate but poor global exploration of the posterior surface.\n",
    "    \n",
    "    - If the acceptance rate is low, the chain is hardly moving meaning that it's stuck in a rut or trying to jump to new points that are too far away.\n",
    "    \n",
    "\n",
    "2. **CHECK TRACEPLOTS:** After getting an idea of the acceptance rate, make traceplots of your chain. Ideally, our tracplot in each parameter would be mixing well (moving across parameter space without getting stuck), and carving out the same patch of parameter space on average. This will tell you whether your chain is getting stuck or encountering inefficiencies.\n",
    "\n",
    "\n",
    "3. **CHECK AUTOCORRELATION LENGTH:** The MCMC chain with Metropolis-Hastings will not give fully-independent random samples. The next point is influenced by where the previous point was. We need to check how much to down-sample the chain so that the points lack memory and influence from others. This is given by the ***autocorrelation length***. \n",
    "\n",
    "\n",
    "Look again at the plots below for an arbitrary problem (my own image). The 1st column is the trace, the 2nd is the histogram of the chain, and the 3rd column is the acceptance rate of newly proposed samples. Familiarize yourself with the kind of inspections needed for MCMC chains.\n",
    "\n",
    "- **In the top row, the proposal width was too small**. \n",
    "- **In the middle row, the proposal width was too big**. \n",
    "- **Only the bottom row shows reasonable sampling. This is the Goldilocks scenario.**\n",
    "\n",
    "![](./figures/fig_taylor_mcmc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next several cells will load up videos from `Vimeo` to demonstrate how Metropolis-Hastings--based MCMC samples (i) a square region, (ii) a diagonal region, (iii) a cross-diagonal region, and finally... (iv) a banana. Because if you can't sample from a banana then what's the use of your MCMC? Execute the cells and watch through them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://player.vimeo.com/video/19274900\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274900\">Metropolis in the Square</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/19274173\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/19275365\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/22616409\" width=\"640\" height=\"480\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe><p><a href=\"https://vimeo.com/19274173\">Metropolis in Diagonal Region</a> from <a href=\"https://vimeo.com/user3812935\">Abraham Flaxman</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing sampling\n",
    "\n",
    "There are several ways of improving the way we propose new sample locations in Metropolis-Hastings. Here are a few.\n",
    "\n",
    "#### (a) Adaptive Metropolis (AM)\n",
    "\n",
    "In AM you use the **empirically-estimated parameter covariance matrix to tune the width of the Gaussian proposal  distribution**. Tuning is updated during the sampling in order to reach optimal mixing. In practice this  means  that  one  uses  the  entire  past  history  of  the  chain  up  until  the  current point to estimate the parameter covariance matrix, scaling this covariance matrix by $\\alpha= 2.38^2/N_\\mathrm{param}$ to reach the optimal $\\sim25\\%$ proposal acceptance rate. \n",
    "\n",
    "Practically speaking, the procedure is\n",
    "- Estimate the $N_\\mathrm{param}\\times N_\\mathrm{param}$ parameter covariance matrix, $C$, using all samples. Standard numpy or scipy algorithms can do this. \n",
    "- Factorize the matrix using a Cholesky algorithm, such that $C = L L^T$.\n",
    "- Draw a new proposed point such that $y = x_i + \\alpha Lu$, where $x_i$ is the current point, and $u$ is an $N_\\mathrm{param}$-dimensional vector of random draws from a zero-mean unit-variance Gaussian.\n",
    "\n",
    "*One subtlety here is that by using more than just the most recent point to tune the sampling, our chain is no longer Markovian. This is easily resolved by allowing the chain to pass through a proposal tuning stage using AM, after which the proposal covariance matrix is frozen so that the chain is Markovian then on.*\n",
    "\n",
    "#### (b) Single Component Adaptive Metropolis (SCAM)\n",
    "\n",
    "With high-dimensional model parameter spaces, or even target posterior distributions with significant covariances amongst some parameters, the AM method may suffer from low acceptance rates. One method that addresses this is a variant on AM called Single Component Adaptive Metropolis (SCAM).  **This  involves  jumping  along selected eigenvectors (or principal axes) of the parameter covariance matrix**, whichis equivalent to jumping in only one uncorrelated parameter at a time. (We'll see more of principal axes later in the course)\n",
    "\n",
    "- We take our parameter covariance matrix as in AM, but this time work out the eigenvalues and eigenvectors, $C = D\\Lambda D^T$, where $D$ is a unitary matrix with eigenvectors as columns, and $\\Lambda = \\mathrm{diag}(\\sigma^2_\\Lambda$) is a diagonal matrix of eigenvalues. \n",
    "- A SCAM jump corresponds to a zero-mean unit-variance jump in a randomly chosen uncorrelated parameter, equivalent to jumping along one of the eigenvectors. \n",
    "- A proposal draw is given by $y = x_i+ 2.4 D_j u_j$, where $D_j$ is a randomly chosen column of D corresponding to the $j$th eigenvector of $C$, and $u_j \\sim \\mathcal{N}(0,\\sigma^j_\\Lambda)$.\n",
    "\n",
    "#### (c) Differential Evolution (DE)\n",
    "\n",
    "Another popular proposal scheme is DE, which is a simple *genetic algorithm* that treats the past history of the  chain up until the current point as a population. \n",
    "\n",
    "- In DE, you choose two random points from the chain’s history to construct a difference vector along which the chain can jump. \n",
    "- A DE proposal draw is given by $y = x_i + \\beta(x_{r1} − x_{r2})$, where $x_{r1,2}$ are parameter vectors from two randomly chosen points in the past history of the chain, and $\\beta$ is a scaling factor that is usually set to be the same as the AM scaling factor, $\\alpha = 2.4^2 / N_\\mathrm{param}$.\n",
    "\n",
    "\n",
    "#### The Full Proposal Cocktail\n",
    "\n",
    "Real world MCMC should use a cocktail of proposal schemes, aimed at ensuring convergence to the target posterior distribution with **minimal burn-in**, **optimal acceptance rate**, and as **short an autocorrelation length** as possible. \n",
    "\n",
    "At each MCMC iteration the proposed parameter location can be drawn according to a weighted list of schemes, involving **(i) AM, (ii) SCAM, (iii) DE, (iv) empirical proposal distributions (e.g. from previous analyses), and finally (iv) draws from the parameter prior distribution**. \n",
    "\n",
    "The final prior-draw scheme allows for occasional large departures from regions of high likelihood, ensuring that we are exploring the full parameter landscape well, and avoiding the possibility of getting stuck in local maxima. \n",
    "\n",
    "Really, you can use any reasonable distribution you like to propose points. Your only constraint is to ensure that detailed balance is maintained through the relevant transition weightings in  the  Metropolis-Hastings ratio, $p_\\mathrm{acc}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC Parameter Estimation In The Wild <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's look at some more involved examples. We will use two popular python modules: `emcee` and `PyMC`. I'll also show you a great but less used sampler that is the standard one for my field of PTA gravitational-wave astrophysics: `PTMCMCSampler`. Quoting Jake VanderPlas for `emcee` and `PyMC`: \n",
    "[Jake's blog:](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/)\n",
    "\n",
    "### emcee\n",
    "\n",
    "> The emcee package (*also known as MCMC Hammer, which is in the running for best Python package name in history*) is a Pure Python package written by Astronomer Dan Foreman-Mackey. It is a lightweight package which implements a fairly sophisticated Affine-invariant Hamiltonian MCMC. Because the package is pure Python (i.e. it contains no compiled extensions) it is extremely easy to install; with pip, simply type at the command-line \"pip install emcee\".\n",
    "\n",
    "Emcee does not have much specific boilerplate code; it simply requires you to pass it a Python function which returns a value proportional to the log-posterior probability, and returns samples from that posterior.*\n",
    "\n",
    "### PyMC\n",
    "\n",
    "> The PyMC package has many more features than emcee, including built-in support for efficient sampling of common prior distributions. PyMC by default uses the classic Metropolis-Hastings sampler, one of the earliest MCMC algorithms. For performance, it uses compiled fortran libraries, so it is less trivial to install using tools like pip. PyMC binaries for many systems can be quite easily installed with conda.*\n",
    "\n",
    "More details about PyMC are available from [the pyMC User Guide](https://pymc-devs.github.io/pymc/), but note that we are going to be using [PyMC3](https://docs.pymc.io/).\n",
    "\n",
    "### PTMCMCSampler\n",
    "\n",
    "> This is a bare-bones sampler, requiring only that the user provide a log-likelihood function and a log-prior function. The user can change the relative amounts of AM, SCAM, and DE being used to propose new points to jump to. The great thing about this sampler is that one can add new custom proposal schemes according to the user's preference. \n",
    "\n",
    "> Also, this sampler implements **parallel tempering MCMC (PTMCMC)**. We won't go into detail about this, but suffice it to say that multiple copies of the MCMC are run in parallel, where the copies actually sample from different roots of the likelihood. The higher roots we take, the more the likelihood is flattened out, making it easier to sample. These \"rooted\" chains then communicate back to the main chain, improving exploration of the parameter space. All these chains can actually be post-processed to get the Bayesian evidence too, in a scheme called **thermodynamic integration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC with emcee <a class=\"anchor\" id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `emcee` and generate some homoescedastic Gaussian data. We'll assume the standard deviation is know, so we're only estimating $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "np.random.seed(21)\n",
    "Ndata = 100\n",
    "mu = 1.0\n",
    "sigma = 0.5 # assumed known \n",
    "data = stats.norm(mu, sigma).rvs(Ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the relevant functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(x, sigma, data):\n",
    "    # Gaussian likelihood \n",
    "    return np.prod(np.exp(-(data-x)**2 /2 /sigma**2))\n",
    "\n",
    "def Prior(x):\n",
    "    return 1.0 / 10   # flat: it cancels out and has no effect \n",
    "\n",
    "def myPosterior(x, sigma, data):\n",
    "    return Likelihood(x, sigma, data) * Prior(x)\n",
    "\n",
    "# emcee wants ln of posterior pdf\n",
    "def myLogPosterior(x, sigma, data):\n",
    "    return np.log(myPosterior(x, sigma, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emcee` combines multiple \"walkers\", each of which is its own MCMC chain. The number of trace results will be nwalkers $\\times$ nsteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 1  # number of parameters in the model\n",
    "nwalkers = 6  # number of MCMC walkers\n",
    "burn = 1000  # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 5000  # number of MCMC steps to take **for each walker**\n",
    "\n",
    "# initialize theta \n",
    "np.random.seed(0)\n",
    "starting_guesses = np.random.random((nwalkers, ndim))\n",
    "\n",
    "# the function call where all the work happens: \n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, myLogPosterior, args=[sigma, data])\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    " \n",
    "# sampler.chain is of shape (nwalkers, nsteps, ndim)\n",
    "# throw-out the burn-in points and reshape:\n",
    "emcee_trace  = sampler.chain[:, burn:, :].reshape(-1, ndim)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.chain.shape) #original chain structure\n",
    "print(emcee_trace.shape) #burned and flattened chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "fig.subplots_adjust(left=0.11, right=0.95, \n",
    "                    wspace=0.35, bottom=0.18)\n",
    "\n",
    "chainE = emcee_trace #[0]\n",
    "M = np.size(chainE)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "xgrid = np.linspace(1, M, M)\n",
    "plt.plot(xgrid, chainE)\n",
    "ax1.axis([0, M, np.min(chainE), 1.1*np.max(chainE)])\n",
    "plt.xlabel('number')\n",
    "plt.ylabel('chain')\n",
    "\n",
    "# plot running mean: \n",
    "meanC = [np.mean(chainE[:int(N)]) for N in xgrid]\n",
    "ax1.plot(xgrid, meanC, c='red', label='chain mean') \n",
    "ax1.plot(xgrid, 0*xgrid + np.mean(data),\n",
    "         c='yellow',label='data mean')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "# skip first burn samples\n",
    "burn = 1000\n",
    "Nchain = np.size(chainE[xgrid>Nburn])\n",
    "Nhist, bins, patches = plt.hist(chainE[xgrid>Nburn], \n",
    "                                bins='auto', histtype='stepfilled')\n",
    "\n",
    "# plot expectations based on central limit theorem\n",
    "binwidth = bins[1] - bins[0]\n",
    "muCLT = np.mean(data)\n",
    "sigCLT = np.std(data)/np.sqrt(Ndata)\n",
    "muGrid = np.linspace(0.7, 1.3, 500)\n",
    "gauss = Nchain * binwidth * stats.norm(muCLT, sigCLT).pdf(muGrid) \n",
    "ax2.plot(muGrid, gauss, c='red') \n",
    "\n",
    "ax2.set_ylabel('p(chain)')\n",
    "ax2.set_xlabel('chain values')\n",
    "ax2.set_xlim(0.7, 1.3)\n",
    "ax2.set_ylim(0, 1.2*np.max(gauss))\n",
    "ax2.set_title(r'Chain from emcee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MCMC with PyMC3 <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Now we will use pyMC3 to get a 2-dimensional posterior pdf for location and scale parameters using a sample drawn from a **Cauchy distribution**. Remember a Cauchy distribution formally has no mean or standard deviation because its tails fall off shallower than $1/x^2$.\n",
    "\n",
    "The following is code adapted from Figure 5.22 of the textbook. Initially, we load in `PyMC3` and define the Cauchy log likelihood. \n",
    "\n",
    "***NOTE:*** The code here for the Cauchy likelihood is actually only for when we look at the analytic estimate, since `PyMC3` has ready-made modules for many standard functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import cauchy\n",
    "import pymc3 as pm\n",
    "\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "def cauchy_logL(xi, sigma, mu):\n",
    "    \"\"\"Equation 5.74: cauchy likelihood\"\"\"\n",
    "    xi = np.asarray(xi)\n",
    "    n = xi.size\n",
    "    shape = np.broadcast(sigma, mu).shape\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple([1 for s in shape]))\n",
    "\n",
    "    return ((n - 1) * np.log(sigma)\n",
    "            - np.sum(np.log(sigma ** 2 + (xi - mu) ** 2), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data from the Cauchy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Draw the sample from a Cauchy distribution\n",
    "np.random.seed(44)\n",
    "mu_0 = 0\n",
    "gamma_0 = 2\n",
    "xi = cauchy(mu_0, gamma_0).rvs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the `PyMC3` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Set up and run MCMC:\n",
    "with pm.Model():\n",
    "    mu = pm.Uniform('mu', -5, 5) #uniform in Cauchy mu\n",
    "    log_gamma = pm.Uniform('log_gamma', -10, 10) #uniform in log of Cauchy gamma\n",
    "\n",
    "    # set up our observed variable x\n",
    "    # i.e. read this as 'x is distributed as a Cauchy variable'\n",
    "    x = pm.Cauchy('x', mu, np.exp(log_gamma), observed=xi)\n",
    "\n",
    "    trace = pm.sample(draws=12000, tune=1000, cores=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have seen some output messages along the lines of \"Auto-assigning NUTS sampler...\". `PyMC3` is very sophisticated, and will automatically decide for you the best tools for the job. \n",
    "\n",
    "In this case, it decided you needed **No-U-Turn Hamiltonian gradient-based sampling**, where gradient information about the Cauchy likelihood helped in the exploration of parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute histogram of results to plot below\n",
    "L_MCMC, mu_bins, gamma_bins = np.histogram2d(trace['mu'],\n",
    "                                             np.exp(trace['log_gamma']),\n",
    "                                             bins=(np.linspace(-5, 5, 41),\n",
    "                                                   np.linspace(0, 5, 41)))\n",
    "L_MCMC[L_MCMC == 0] = 1E-16  # prevents zero-division errors\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Compute likelihood analytically for comparison\n",
    "mu = np.linspace(-5, 5, 70)\n",
    "gamma = np.linspace(0.1, 5, 70)\n",
    "logL = cauchy_logL(xi, gamma[:, np.newaxis], mu)\n",
    "logL -= logL.max()\n",
    "\n",
    "p_mu = np.exp(logL).sum(0)\n",
    "p_mu /= p_mu.sum() * (mu[1] - mu[0])\n",
    "\n",
    "p_gamma = np.exp(logL).sum(1)\n",
    "p_gamma /= p_gamma.sum() * (gamma[1] - gamma[0])\n",
    "\n",
    "hist_mu, bins_mu = np.histogram(trace['mu'], bins=mu_bins, density=True)\n",
    "hist_gamma, bins_gamma = np.histogram(np.exp(trace['log_gamma']),\n",
    "                                      bins=gamma_bins, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# plot all the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# first axis: likelihood contours\n",
    "ax1 = fig.add_axes((0.4, 0.4, 0.55, 0.55))\n",
    "ax1.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax1.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "ax1.contour(mu, gamma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='b', linestyles='dashed')\n",
    "\n",
    "ax1.contour(0.5 * (mu_bins[:-1] + mu_bins[1:]),\n",
    "            0.5 * (gamma_bins[:-1] + gamma_bins[1:]),\n",
    "            convert_to_stdev(np.log(L_MCMC.T)),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.1, 0.4, 0.29, 0.55))\n",
    "ax2.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax2.plot(hist_gamma, 0.5 * (bins_gamma[1:] + bins_gamma[:-1]\n",
    "                            - bins_gamma[1] + bins_gamma[0]),\n",
    "         '-k', drawstyle='steps')\n",
    "ax2.plot(p_gamma, gamma, '--b')\n",
    "ax2.set_ylabel(r'$\\gamma$')\n",
    "ax2.set_ylim(0, 5)\n",
    "\n",
    "# third axis: marginalized over gamma\n",
    "ax3 = fig.add_axes((0.4, 0.1, 0.55, 0.29))\n",
    "ax3.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "ax3.plot(0.5 * (bins_mu[1:] + bins_mu[:-1]), hist_mu,\n",
    "         '-k', drawstyle='steps-mid')\n",
    "ax3.plot(mu, p_mu, '--b')\n",
    "ax3.set_xlabel(r'$\\mu$')\n",
    "plt.xlim(-5, 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting $2$D joint posterior pdf corner plot shows analytic results as blue dashed lines, and MCMC sampling results as black contours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC With The PTMCMCSampler <a class=\"anchor\" id=\"five\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now explore the more bare-bones sampler. This helps to expose a lot of what is going on in the other samplers, where it's difficult to see what happens under the hood.\n",
    "\n",
    "**We're going to analyze a homoescedastic Gaussian dataset, but this time search over the mean and standard deviation with our MCMC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't worry about any warning messages regarding mpi4py\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "Ndata = 100\n",
    "mu = 1.0\n",
    "sigma = 0.5 # assumed known \n",
    "data = stats.norm(mu, sigma).rvs(Ndata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function below to create a prior function that is the multiplication of a uniform prior for the mean over $0.5$ to $1.5$, and a uniform prior for the standard deviation between $0.1$ and $0.9$. \n",
    "\n",
    "The priors are separable and independent, so the overall prior should be the product of the prior for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Likelihood(x, data):\n",
    "    # Gaussian likelihood \n",
    "    return np.prod(stats.norm(loc=x[0], scale=x[1]).pdf(data))\n",
    "\n",
    "def Prior(x):\n",
    "    # create a uniform prior in mu and sigma\n",
    "    return ___\n",
    "\n",
    "def logLikelihood(x):\n",
    "    return np.log(Likelihood(x, data))\n",
    "\n",
    "def logPrior(x):\n",
    "    return np.log(Prior(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of parameter space\n",
    "ndim = 2\n",
    "\n",
    "# initial jump covariance matrix\n",
    "cov = np.diag(np.ones(ndim) * 0.01**2)\n",
    "\n",
    "# intialize sampler\n",
    "sampler = ptmcmc(ndim, logLikelihood, logPrior, cov, \n",
    "                 outDir='./my_ptmcmc_chain', resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for N steps\n",
    "N = int(3e4)\n",
    "x0 = np.array([0.9, 0.3])\n",
    "sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the chain\n",
    "chain = np.loadtxt('./my_ptmcmc_chain/chain_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>OK, now that you have your chain, it's up to you to diagnose it.</font>\n",
    "    \n",
    "- Make traceplots in both $\\mu$ and $\\sigma$. \n",
    "- Discuss when you should cut off burn-in. \n",
    "- Compute the autocorrelation length of the chain, and down-sample it to select only points every autocorrelation length. \n",
    "- Finally, make a corner plot that has labels, shows titles, and has $68\\%$ and $95\\%$ quantiles and levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acor\n",
    "acor.acor(chain[:,0]) \n",
    "# the first element of the tuple is the length between independent samples.\n",
    "# thin by the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:astr8070] *",
   "language": "python",
   "name": "conda-env-astr8070-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
