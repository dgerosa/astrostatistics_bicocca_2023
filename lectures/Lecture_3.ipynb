{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Probability & Statistics: II\n",
    "\n",
    "*S. R. Taylor (2021)*\n",
    "\n",
    "Material in this lecture and notebook is based upon the Basic Stats portion of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), the Introduction to Probability & Statistics portion of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18), and J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html). \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 3.\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Descriptive statistics](#one)\n",
    "* [Sample versus population statistics](#two)\n",
    "* [Univariate distributions](#three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "As we've said, our goal is to estimate $h(x)$ given some measured data, allowing us to reconstruct the data-based distribution $f(x)$. An arbitrary distribution can be characterized by **location** parameters (i.e., position), **scale** parameters (i.e., width), and **shape** parameters. These parameters are called ***descriptive statistics***.\n",
    "\n",
    "The distribution we're trying to characterize could be anything, e.g. (from my field) the distribution of masses of binary black-hole systems as discovered by gravitational-wave detectors. We really don't know the answer to this well, and the problem is made more complicated by things like detector selection effects (heavier systems are more likely to be observed), and blurring effects from measurement precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from astroML import stats as astroMLstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "#------------------------------------------------------------\n",
    "# Let's generate some data: a mix of several Cauchy distributions\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 10000\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "hx = lambda x: sum([f * scipy.stats.cauchy(mu, gamma).pdf(x)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "data = np.concatenate([scipy.stats.cauchy(mu, gamma).rvs(int(f * N), \n",
    "                                                         random_state=random_state)\n",
    "                       for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(data)\n",
    "data = data[data > -10]\n",
    "data = data[data < 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# make a histogram to get an idea of what the distribution looks like\n",
    "plt.hist(data, bins=50, density=True, alpha=0.5);\n",
    "plt.xlabel('$x$');\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that the **mean** of a sample is \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$$ \n",
    "\n",
    "This is actually known as the **sample arithmetic mean**, and derives from *Monte Carlo integration* to get the first moment of the distribution, i.e. \n",
    "\n",
    "$$\\mu = E(x) = \\langle x \\rangle = \\int_{-\\infty}^{\\infty} x h(x)\\,dx \\approx \\frac{1}{N}\\sum_{i=1}^N x_i $$\n",
    "\n",
    "where $\\{x_i\\}$ are random samples from the properly normalzied $h(x)$, and $E(\\cdot)$ means the **expectation value**. In general we can use random sampling and Monte Carlo integration to deduce integrals over distributions such that \n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} g(x) h(x)\\,dx \\approx \\frac{1}{N}\\sum_{i=1}^N g(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mean = np.mean(data)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's most common to compute the mean, it may surprise you to learn that some distributions do not have formally calculable means (integration gives infinity). In these and other cases, the **median** is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by **outliers**.\n",
    "\n",
    "To understand the previous statement, think about multiplying all numbers above the 50th percentile (i.e. the median) by 100, or even just replacing them with larger numbers. The mean would be strongly affected by these corrupted points, but **cumulative statistics based on the ordering of samples would remain unaffected by the outlier corruption**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell.  Think about and discuss what it is doing.\n",
    "median = np.median(data)\n",
    "\n",
    "mask = data > 15\n",
    "data2 = data.copy()\n",
    "data2[mask] = 100\n",
    "\n",
    "newmedian = np.median(data2)\n",
    "newmean = np.mean(data2)\n",
    "\n",
    "print(median, newmedian)\n",
    "print(mean, newmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Repeat the above masking investigation, but this time multiply all samples above $15$ by a factor of 10. Do you get a similar effect?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other descriptive statistics are related to higher order moments of the distribution. Beyond the \"average\" *location* value, we'd like to know something about **deviations** from the average (which is related to the *shape* of the distribution).  The simplest thing to compute is $$d_i = x_i - \\mu.$$  However, the average deviation is zero by definition of the mean.  The next simplest thing to do is to compute the **mean absolute deviation (MAD)**:\n",
    "\n",
    "$$\\frac{1}{N}\\sum|x_i-\\mu|,$$\n",
    "\n",
    "but the absolute values can hide the true scatter of the distribution [in some cases (see footnote)](http://www.mathsisfun.com/data/standard-deviation.html).  So the next simplest thing to do is to square the differences $$\\sigma^2 = \\frac{1}{N}\\sum(x_i-\\mu)^2,$$ which we call the **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *variance* $V$ is just expectation value of $(x-\\mu)^2$ (and related to the 2nd moment)\n",
    "\n",
    "$$\\sigma^2 = V = E((x-\\mu)^2)\\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where $\\sigma$ is the **standard deviation**. Again, the integral gets replaced by a sum for discrete distributions. While most familiar for Gaussian distributions, you can compute the variance even if your distribution is not Gaussian.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "var = np.var(data)\n",
    "std = np.std(data)\n",
    "print(var, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the **Median Absolute Deviation (also MAD)** given by\n",
    "\n",
    "$${\\rm median} (|x_i-{\\rm median}(\\{x_i\\})|)$$\n",
    "\n",
    "where $\\sigma = 1.4826\\,{\\rm MAD}$ for a Gaussian distribution (but note that we aren't using a Gaussian distribution above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from astropy.stats import median_absolute_deviation\n",
    "MAD = median_absolute_deviation(data)\n",
    "print(MAD,MAD*1.4826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$P\\%$ quantiles (or the $p^\\mathrm{th}$ percentile, $q_p$)** are computed as\n",
    "$$\\frac{p}{100} = H(q_p) = \\int_{-\\infty}^{q_p}h(x) dx$$\n",
    "\n",
    "The full integral from $-\\infty$ to $\\infty$ is 1 (100%).  So, here you are looking for the value of x that accounts for $p$ percent of the distribution.\n",
    "\n",
    "For example, the 25th, 50th, and 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "q25, q50, q75 = np.percentile(data, [25, 50, 75])\n",
    "print(q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **interquartile range** is the difference between the 25th and 75th percentiles, $q_{75} - q_{25}$.\n",
    "\n",
    "Just as with the median, the interquartile range is a more *robust* estimator of the scale of a distribution than the standard deviation.  So, one can create a standard-deviation-esque measurement (at least for a Gaussian) from the interquartile range as\n",
    "\n",
    "$$\\sigma_G = 0.7413\\times(q_{75} - q_{25})$$  \n",
    "\n",
    "The normalization makes it *unbiased* for a perfect Gaussian (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell. Think about and discuss the results.\n",
    "from astroML import stats as astroMLstats\n",
    "\n",
    "# original data\n",
    "print(astroMLstats.sigmaG(data), np.std(data))\n",
    "\n",
    "# corrupted by outliers\n",
    "print(astroMLstats.sigmaG(data2), np.std(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell. Cumulative statistics take longer to compute, but are more robust.\n",
    "%timeit np.mean(data), np.std(data)\n",
    "%timeit np.median(data), astroMLstats.sigmaG(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Make a plot of a histogram of the original data array, and add vertical lines at the 25th, 50th, and 75th percentiles.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mode** is the most probable value, determined from the peak of the distribution, which is the value where the derivative is 0 (i.e. the turning point):\n",
    "\n",
    "$$ \\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$\n",
    "\n",
    "Another way to estimate the mode (at least for a Gaussian distribution) is\n",
    "\n",
    "$$x_m = 3q_{50} - 2\\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mode = 3*q50 - 2*mean\n",
    "print(mode, mean, median)\n",
    "\n",
    "# Note: don't rely on scipy.stats.mode()\n",
    "# It gives the most common value of an array, \n",
    "# but we have a random sample of unique draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful ***shape*** measures include the \"higher order\" moments (the **skewness** and **kurtosis**):\n",
    "\n",
    "$$\\mathbf{Skewness}\\quad\\quad \\Sigma = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^3 h(x) dx,$$\n",
    " \n",
    "$$\\mathbf{Kurtosis}\\quad\\quad K = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx  - 3.$$\n",
    "\n",
    "The skewness measures the distribution's *asymmetry*. Distribution's with long tails to larger $x$ values have positive $\\Sigma$. \n",
    "\n",
    "The kurtosis measures how peaked or flat-topped a distribution is, with strongly peaked ones being positive and flat-topped ones being negative. $K$ is calibrated to a Gaussian distribution (hence the subtraction of $3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://www.astroml.org/_images/fig_kurtosis_skew_1.png](https://www.astroml.org/_images/fig_kurtosis_skew_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "skew = scipy.stats.skew(data)\n",
    "kurt = scipy.stats.kurtosis(data)\n",
    "print(skew, kurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excute this cell\n",
    "# Summary descriptive statistics for our distribution\n",
    "print(\"Location: \", mean, median, mode)\n",
    "print(\"Scale: \", var, std, astroMLstats.sigmaG(data))\n",
    "print(\"Shape: \", skew, kurt)\n",
    "print(\"Some percentiles: \", q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample versus Population statistics <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Statistics estimated from the *data* are called **sample statistics** as compared to **population statistics** derived from knowing the functional form of the pdf.\n",
    "\n",
    "Specifically, $\\mu$ is the **population mean**, i.e., it is the expecation value of $x$ for $h(x)$.  But we don't *know* $h(x)$.  So the **sample mean**, $\\overline{x}$, is an ***estimator*** of $\\mu$, defined as\n",
    "\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "\n",
    "which we determine from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the **population variance** $\\sigma^2$, we have the **sample variance**, $s^2$, where\n",
    "\n",
    "$$s^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\overline{x})^2$$\n",
    "\n",
    "The $N-1$ denominator (instead of $N$) accounts for the fact that we determine $\\overline{x}$ from the data itself instead of using a known $\\mu$. Ideally one tries to work in a regime where $N$ is large enough that we can be lazy and ignore this. \n",
    "\n",
    "So the mean and variance of a distribution are $\\mu$ and $\\sigma^2$.  The *estimators* of the distribution are $\\overline{x}$ (or $\\hat{x}$) and $s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty of sample statistics\n",
    "\n",
    "We would also like to know the uncertainty of our estimates $\\overline{x}$ and $s$.  Note that $s$ is the width estimate of the underlying distribution; it is **NOT** the uncertainty of $\\overline{x}$. Rather the uncertainty of $\\overline{x}$, $\\sigma_{\\overline{x}}$ is \n",
    "\n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "\n",
    "which we call the **standard error of the mean**. The uncertainty of $s$ itself is\n",
    "\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$.\n",
    "\n",
    "Another useful uncertainty estimate is for computing quantiles. The **standard error of a quantile** is\n",
    "\n",
    "$$ \\sigma_{q_p} = \\frac{1}{h_p}\\sqrt{\\frac{p(1-p)}{N}}$$\n",
    "\n",
    "where $p$ is between $0$ and $1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Univariate distributions <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "If we are attempting to characterize our data in a way that is **parameterized**, then we need a functional form for a **distribution**.  There are many naturally occurring distributions.  The book goes through quite a few of them.  Here we'll just talk about a few basic ones to get us started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell (don't worry about warnings)\n",
    "# Since you're working with a copy of my notebook, you\n",
    "# may need to change the path below to find the file\n",
    "%matplotlib inline\n",
    "%run ./code/fig_uniform_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement [uniform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html#scipy.stats.uniform) in `scipy` as follows.  We'll use the methods listed at the bottom of the link to complete the cell: `dist.rvs(size=N)` which produces `N` random draws from the distribution and `dist.pdf(x)` which returns the value of the pdf at a given $x$. Lots of distributions can be accessed and used in a similar way.  \n",
    "\n",
    "Create a uniform distribution with parameters `loc=0`,  `scale=2`, and `N=10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = ____ # Complete\n",
    "distU = scipy.stats.uniform(____,____) # Complete\n",
    "draws = distU.rvs(____) # ten random draws\n",
    "print(draws)\n",
    "\n",
    "p = distU.pdf(____) # pdf evaluated at x=1\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "As many of you know, the Gaussian distribution pdf is given by\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "It is also called the **normal distribution** and can be noted by $\\mathscr{N}(\\mu,\\sigma)$.\n",
    "\n",
    "\n",
    "We love using Gaussians in physics and astronomy because they can approximate many distributions and are also super easy to work with. **The convolution of two Gaussians results in a Gaussian.**  So $\\mathscr{N}(\\mu_1,\\sigma_1)$ convolved with $\\mathscr{N}(\\mu_2,\\sigma_2)$ is $\\mathscr{N}(\\mu_1+\\mu_2,\\sqrt{\\sigma_1^2+\\sigma_2^2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ./code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the next line and run this cell; I just want you to know that this magic function exists.\n",
    "# %load ../code/fig_gaussian_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [normal distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=stats%20norm#scipy.stats.norm) with `loc=100` and `scale=15`. Produce 10 random draws and determine the probability at `x=145`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distG = scipy.stats.norm(____,____) # Normal distribution with mean = 100, stdev = 15\n",
    "draws = ____.____(____) # 10 random draws\n",
    "p = ____.____(____) # pdf evaluated at x=0\n",
    "\n",
    "print(draws)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of this Gaussian distribution. Plot the pdf from 0 to 200 with a 1000 gridpoints.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's play with Gaussians! Or Normal distributions, N(mu,sigma)\n",
    "\n",
    "xgrid = np.linspace(____,____,____) # generate distribution for a uniform grid of x values\n",
    "____ = distG.pdf(____)  # this is a function of xgrid\n",
    "\n",
    "# actual plotting\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "\n",
    "# Python3 f strings are awesome!\n",
    "plt.plot(xgrid, gaussPDF, ls='-', c='black', \n",
    "         label=f'$\\mu={mu},\\ \\sigma={sigma}$')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 0.03)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative distribution function, cdf is the integral of pdf from $x'=-\\infty$ to $x'=x$:\n",
    "\n",
    "$$\\mathrm{cdf}(x|\\mu,\\sigma) = \\int_{-\\infty}^{x'} p(x'|\\mu,\\sigma) dx',$$\n",
    "\n",
    "where $\\mathrm{cdf}(\\infty) = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same as above but now with the cdf method\n",
    "gaussCDF = distG.cdf(xgrid)\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "plt.plot(xgrid, gaussCDF, ls='-', c='black', \n",
    "         label=r'$\\mu=%i,\\ \\sigma=%i$' % (mu, sigma))\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(-0.01, 1.01)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$CDF(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "\n",
    "- For $a=b=1\\sigma$, we get the familar result of 68.3%.  \n",
    "- For $a=b=2\\sigma$ it is 95.4%.  \n",
    "- For $a=b=3\\sigma$ it is $99.7\\%$. \n",
    "\n",
    "So we refer to the range $\\mu \\pm 1\\sigma$, $\\mu \\pm 2\\sigma$, and $\\mu \\pm 3\\sigma$ as the 68%, 95%, and $99.7%$ **confidence limits**, respectively. Note that if your distribution is not Gaussian, then these confidence intervals will be different!\n",
    "\n",
    "***We often still refer to uncertainty regions of distributions as $1\\sigma$ or $2\\sigma$ regions, which for non-Gaussian distributions usually means (for $1\\sigma$) the region enclosing the $16\\%$ and $84\\%$ quantiles.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability enclosed between $-2\\sigma$ and $+4\\sigma$? (*Verify first that you get the correct answer for the bullet points above!*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "mu=0\n",
    "sigma=1\n",
    "distN = ___.___.___(mu, sigma) # Complete\n",
    "xgrid = np.linspace(___,___,N) # Complete\n",
    "dx = (xgrid.max()-xgrid.min())/N\n",
    "prob = distN.pdf(xgrid)*dx\n",
    "\n",
    "print(prob.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do this in various ways. The way you just tried was the most obvious-- brute-force numerical integration with the trapezoidal method. \n",
    "\n",
    "But the clever way is to use the cdf, by computing the cdf of the upper integration bound and subtracting the cdf of the lower integration bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = distN.cdf(___)\n",
    "lower = distN.cdf(___)\n",
    "p = upper-lower\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "Note that if $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$, the median is $\\exp(\\mu)$, and the mode is $\\exp(\\mu-\\sigma^2)$.  Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "x = scipy.stats.norm(0,1) # mean = 0, stdev = 1\n",
    "y = np.exp(x.rvs(100))\n",
    "\n",
    "print(y.mean())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catch here is that stats.norm(0,1) returns an *object* and not something that we can just do math on in the expected manner.  What *can* you do with it?  Try ```dir(x)``` to get a list of all the methods and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "distLN = scipy.stats.norm(0,1) # mean = 0, stdev = 1\n",
    "x = distLN.rvs(10000)\n",
    "y = np.exp(x)\n",
    "\n",
    "print(np.exp(0 + 0.5*1), y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Repeat the above calculations to verify the equations for the mode and median.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next chapter.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ./code/fig_chi2_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared per degree of freedom\n",
    "\n",
    "In practice we frequently divide $\\chi^2$ by the number of degrees of freedom, and work with:\n",
    "\n",
    "$$\\chi^2_\\mathrm{dof} = \\frac{1}{N-1} \\sum_{i=1}^N \\left(\\frac{x_i-\\overline{x}}{\\sigma}\\right)^2$$\n",
    "\n",
    "which (for large $k$) is distributed as\n",
    "\n",
    "$$ p(\\chi^2_\\mathrm{dof}) \\sim \\mathscr{N}\\left(1, \\sqrt{\\frac{2}{N-1}}\\right) $$\n",
    "\n",
    "(where $k = N-1$, and $N$ is the number of samples). Therefore, we expect $\\chi^2_\\mathrm{dof}$ to be 1, to within a few $\\sqrt{\\frac{2}{N-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "This is a distribution for a discrete variable, telling you the probability of $k$ events occuring within a certain time when the mean is $\\mu$. \n",
    "\n",
    "An early and famous example of the use of this distribution was to **model the expected number of Prussian cavalrymen that would be kicked to death by their own horse**. Statistics has many applications...\n",
    "\n",
    "$$ p(k|\\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!} $$\n",
    "\n",
    "where the mean $\\mu$ completely characterizes the distribution. The mode is $(\\mu-1)$, the standard deviation is $\\sqrt{\\mu}$, the skewness is $1/\\sqrt{\\mu}$, and the kurtosis is $1/\\mu$.\n",
    "\n",
    "As $\\mu$ increases the Poisson distribution becomes more and more similar to a Gaussian with $\\mathcal{N}(\\mu,\\sqrt{\\mu})$. The Poisson distribution is sometimes called the ***law of small numbers*** or ***law of rare events***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell for $\\mu=3$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice the Poisson distribution\n",
    "\n",
    "dist = scipy.stats.___(___)\n",
    "\n",
    "k = dist.rvs(___) # make 20 draws\n",
    "pmf = dist.___(6) # evaluate probability mass function at 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom):\n",
    "\n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails.\n",
    "\n",
    "Note the similarity between $t$ and $z$ for a Gaussian (as defined in the $\\chi^2$ section above), which reflects the difference between data-derived estimates of the mean and standard deviation and their true values.\n",
    "\n",
    "In fact, although often approximated as a Gaussian distribution, the mean of a sample actually follows a Student's $t$ distribution. This matters when sample sizes are small, but mostly irrelevant for \"Big Data\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "%run ./code/fig_student_t_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the point of all these distributions?\n",
    "\n",
    "* There are many other distributions that we haven't covered here (see the textbook).\n",
    "* The point is that we are going to make some measurement. \n",
    "* To understand the significance of our measurement, we want to know how likely it is that we would get that measurement in our experiment by random chance. \n",
    "* To determine that we need to know the shape of the distribution. Let's say that we find that $x=6$. If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  \n",
    "\n",
    "Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUN FACT:** \"Student\" was the pen name of W. S. Gosset, who worked for the Guinness brewery in Dublin, Ireland. He was interested in the statistical analysis of small samples, e.g., the chemical properties of barley when the sample size might be as small as $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg](https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:astr8070] *",
   "language": "python",
   "name": "conda-env-astr8070-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
