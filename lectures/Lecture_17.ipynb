{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression: I\n",
    "\n",
    "*S. R. Taylor (2021)*\n",
    "\n",
    "This lecture and notebook are based on the \"Regression\" and \"Regression2\" lectures of of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), which in turn are based on materials from Andy Connolly, and Ivezic et al. Chapter 8.\n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 8.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "* [What is regression?](#one)\n",
    "* [2-D linear regression](#two)\n",
    "* [Multivariate linear regression](#three)\n",
    "* [Polynomial regression](#four)\n",
    "* [Basis function regression](#five)\n",
    "* [Kernel regression](#six)\n",
    "* [Over/under-fitting](#seven)\n",
    "* [Cross-validation](#eight)\n",
    "    \n",
    "---\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is regression? <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "[**Regression**](https://en.wikipedia.org/wiki/Regression_analysis) is about determining the relationship between an independent variable, $x$, and the variable that depends on it, $y$, where the expectation value of $y$ is $E[y|x]$. Crudely speaking, it is the set of techniques associated with curve fitting. In contrast to density estimation, clustering, and dimensional reduction (which was largely *unsupervised*) **regression is a *supervised* process**.\n",
    "\n",
    "- Generally, we'd like to infer the true generating pdf of observations from a multi-dimensional sample of data that is drawn from that pdf, using parametric or non-parametric models to do so.  \n",
    "- This is hard, so ***regression seeks to determine the expectation value of $y$ (given $x$, i.e. the conditional expectation value) rather than the full pdf.***  \n",
    "- That is, for a given value of $x$ you'd like to predict the pdf of $y$, but you are going to settle for a **point estimate** (single value).\n",
    "\n",
    "When I say \"regression\", you probably think about linear least-squares fitting (fitting a line) or maximum likelihood analysis. However, adopting a Bayesian perspective enables a more physical intuition that includes how we can do regression in the case of both errors and limits on the data. \n",
    "\n",
    "Let's start by looking at the classic example of fitting a straight line to some data points in 2-D as illustrated by Ivezic, Figure 8.1:\n",
    "\n",
    "![Ivezic, Figure 8.1a](http://www.astroml.org/_images/fig_linreg_inline_1.png)\n",
    "\n",
    "- Here we have 4 data points $\\{x_1,x_2,x_3,x_4\\}$ drawn from $y=\\theta_1 x + \\theta_0$, where $\\theta_1 = 1$ and $\\theta_0 = 0$.  \n",
    "- Each data point provides a joint constraint on $(\\theta_0,\\theta_1)$.  \n",
    "- If there were no uncertainties in the measurement of each $y$, then each new point would yield a straight line constraint in $(\\theta_0,\\theta_1)$ of \n",
    "$\\theta_0 = y_i - \\theta_1 x_i$.  \n",
    "- Think about the constraints from the first point. You could fit an infinite number of different lines through it, all of which would completely cover the $(x,y)$ plane. ***However the parameters of those infinite lines are constrained by $x_1$ to lie along a unique line in the $(\\theta_0,\\theta_1)$ plane.*** See the first panel in the plot from the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.1, modified by SRT and GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set up the data and errors\n",
    "np.random.seed(13)\n",
    "a = 1\n",
    "b = 0\n",
    "\n",
    "x = np.array([-1, 0.44, -0.16, 1.0])\n",
    "y = a * x + b\n",
    "dy = np.array([0.01, 0.01, 0.01, 0.01])\n",
    "\n",
    "y = np.random.normal(y, dy)\n",
    "\n",
    "# add a fourth point which is a lower bound\n",
    "x5 = 1.0\n",
    "y5 = a * x5 + b + 0.0 \n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Compute the likelihoods for each point\n",
    "a_range = np.linspace(0, 2, 80)\n",
    "b_range = np.linspace(-1, 1, 80)\n",
    "logL = -((a_range[:, None, None] * x + \n",
    "          b_range[None, :, None] - y) / dy) ** 2\n",
    "sigma = [convert_to_stdev(logL[:, :, i]) for i in range(4)]\n",
    "\n",
    "# compute best-fit from first three points\n",
    "logL_together = logL.sum(-1)\n",
    "i, j = np.where(logL_together == np.max(logL_together))\n",
    "\n",
    "amax = a_range[i[0]]\n",
    "bmax = b_range[j[0]]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the first figure: the points and errorbars\n",
    "fig1 = plt.figure(figsize=(6, 4))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "\n",
    "# Draw the true and best-fit lines\n",
    "xfit = np.array([-1.5, 1.5])\n",
    "ax1.plot(xfit, a * xfit + b, ':k', label='True fit')\n",
    "ax1.plot(xfit, amax * xfit + bmax, '--k', \n",
    "         label='fit to $\\{x_1, x_2, x_3\\}$')\n",
    "\n",
    "ax1.legend(loc=2)\n",
    "\n",
    "ax1.errorbar(x, y, dy, fmt='ok')\n",
    "ax1.errorbar([x5], [y5], [[0.5], [0]], \n",
    "             fmt='_k', uplims=True)\n",
    "\n",
    "for i in range(4):\n",
    "    ax1.text(x[i] + 0.05, y[i] - 0.3, \n",
    "             \"$x_{%i}$\" % (i + 1))\n",
    "ax1.text(x5 + 0.05, y5 - 0.5, \"$x_4$\")\n",
    "\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$y$')\n",
    "\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-2, 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the second figure: likelihoods for each point\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "fig2.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot likelihood contours\n",
    "for i in range(5):\n",
    "    ax = fig2.add_subplot(321 + i)\n",
    "    for j in range(min(i + 1, 4)):\n",
    "        ax.contourf(a_range, b_range, sigma[j].T,\n",
    "                    levels=(0, 0.683, 0.955, 0.997),\n",
    "                    cmap=plt.cm.binary, alpha=0.5)\n",
    "\n",
    "# plot the excluded area from the fourth point\n",
    "axpb = a_range[:, None] * x5 + b_range[None, :]\n",
    "mask = y5 < axpb\n",
    "fig2.axes[4].fill_between(a_range, y5 - x5 * a_range, \n",
    "                          2, color='k', alpha=0.5)\n",
    "\n",
    "# Label and adjust axes\n",
    "for i in range(5):\n",
    "    ax = fig2.axes[i]\n",
    "\n",
    "    ax.text(1.98, -0.98, \"$x_{%i}$\" % (i + 1), \n",
    "            ha='right', va='bottom')\n",
    "\n",
    "    ax.plot([0, 2], [0, 0], ':k', lw=1)\n",
    "    ax.plot([1, 1], [-1, 1], ':k', lw=1)\n",
    "\n",
    "    ax.set_xlim(0.001, 2)\n",
    "    ax.set_ylim(-0.999, 1)\n",
    "\n",
    "    if i in (1, 3):\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    if i in (0, 1, 2):\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    if i in (0, 2):\n",
    "        ax.set_ylabel(r'$\\theta_0$')\n",
    "    if i in (3, 4):\n",
    "        ax.set_xlabel(r'$\\theta_1$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Now look at the top right panel. The addition of another data point is sufficient to fully constrain the properties of a linear relationship, shown by the intersection of the $(\\theta_0,\\theta_1)$ lines from each point.\n",
    "- More data points yield more constraints, and the best fit solution is the intersection of all lines in model parameter space (as shown in the next 3 panels).\n",
    "\n",
    "**NOTE:** The errors are really small so that the constraints are just lines in parameter space. The 4th $x$ value is shown as a point estimate and a limit, so that you can see that the excluded region from the limit just follows the line that would result had it been a detection.\n",
    "\n",
    "If the measurements have uncertainties then the lines are linear-shaped distributions as illustrated in the actual Figure 8.1 from Ivezic as shown below.\n",
    "\n",
    "![Ivezic, Figure 8.1b](http://www.astroml.org/_images/fig_linreg_inline_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian perspective on regression\n",
    "\n",
    "If we take a Bayesian approach to regression, then we can easily write the posterior pdf for the model parameters as\n",
    "\n",
    "$$p(\\theta|\\{x_i, y_i\\},I) \\propto p(\\{x_i,y_i\\} | \\theta, I) \\, p(\\theta, I),$$\n",
    "\n",
    "where the data likelihood is the product over the individual event likelihoods, which can be written as\n",
    "\n",
    "$$p(y_i|x_i,{\\theta}, I) = e(y_i|y)$$\n",
    "\n",
    "with $e(y_i|y)$ being the probability of getting $y_i$ given the adopted model of $y=f(x|\\theta)$ (i.e. the error distribution). If the error distribution is Gaussian then,\n",
    "\n",
    "$$p(y_i|x_i,{\\theta}, I) = {1 \\over \\sigma_i \\sqrt{2\\pi}} \\, \\exp{\\left({-[y_i-f(x_i|{\\theta})]^2 \\over 2 \\sigma_i^2}\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2-D Linear Regression <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's start with the simplest case: a linear model with independent variable, $x$, and dependent variable, $y$:\n",
    "\n",
    "$$y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i,$$\n",
    "\n",
    "where $\\theta_0$ and $\\theta_1$ are the coefficients of the model that we are trying to estimate, and $\\epsilon_i$ is an additive noise term with $\\epsilon_i = \\mathscr{N}(0,\\sigma_i)$. \n",
    "\n",
    "As we've seen many times, the full data likelihood is the product of each individual event likelihood:\n",
    "\n",
    "$$p(\\{y_i\\}|\\{x_i\\},{\\theta}, I) \\propto \\prod_{i=1}^N \\exp \\left(\\frac{-(y_i- (\\theta_0 + \\theta_1x_{i}))^2}{  2\\sigma_i^2}\\right).$$\n",
    "\n",
    "Assuming a flat/uninformative prior, the log of the posterior probability is then\n",
    "\n",
    "$$\\ln(p({\\theta}|\\{x_i, y_i\\},I)) \\propto \\ln \\mathcal(L)  \\propto \\sum_{i=1}^N \\left(\\frac{-(y_i- (\\theta_0 + \\theta_1x_{i}))^2}{  2\\sigma_i^2}\\right).$$\n",
    "\n",
    "***We want to find the values of $\\theta$ that maximize this expression, which is the same as minimizing the least squares.*** \n",
    "\n",
    "### Homoscedastic uncertainty scenario\n",
    "\n",
    "With uncertainties that are the same for all points, this minimization yields\n",
    "\n",
    "$$\\theta_1 = \\frac{\\sum_i^N x_i y_i - \\bar{x}\\bar{y}}{\\sum_i^N(x_i-\\overline{x})^2},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\theta_0 = \\overline{y} - \\theta_1\\overline{x},$$\n",
    "\n",
    "where $\\overline{x}$ and $\\overline{y}$ are the mean values.\n",
    "\n",
    "The estimate of the variance and the standard errors of the estimated parameters are\n",
    "\n",
    "$$\\sigma^2 = \\sum_{i=1}^N (y_i - \\theta_0 + \\theta_1 x_i)^2,$$\n",
    "\n",
    "$$\\sigma_{\\theta_1}^2 = \\sigma^2\\frac{1}{\\sum_i^N(x_i-\\overline{x})^2},$$\n",
    "\n",
    "$$\\sigma_{\\theta_0}^2 = \\sigma^2\\left(\\frac{1}{N} + \\frac{\\overline{x}^2}{\\sum_i^N(x_i-\\overline{x})^2}\\right).$$\n",
    "\n",
    "### Heteroscedastic uncertainty scenario\n",
    "\n",
    "If the errors are instead different for each point, it is better to think of the problem in matrix notation:\n",
    "\n",
    "$$Y = M \\theta$$\n",
    "\n",
    "where $Y$ is an $N$-dimensional vector of values ${y_i}$,\n",
    "\n",
    "$$Y = \\left[\n",
    "    \\begin{array}{c}\n",
    "    y_0\\\\\n",
    "    \\vdots\\\\\n",
    "    y_{N-1}\n",
    "    \\end{array}\n",
    "    \\right].\n",
    "$$\n",
    "\n",
    "For the straight line model, $\\theta$ is simply a two-dimensional vector of regression coefficients,\n",
    "\n",
    "$$\n",
    "\\theta = \\left[\n",
    "            \\begin{array}{c}\n",
    "            \\theta_0\\\\\n",
    "            \\theta_1\n",
    "            \\end{array}\n",
    "          \\right],\n",
    "$$\n",
    "\n",
    "and **$M$ is a called the design matrix**\n",
    "\n",
    "$$\n",
    "M = \\left[\n",
    "        \\begin{array}{cc}\n",
    "        1 & x_0\\\\\n",
    "        \\vdots & \\vdots\\\\\n",
    "        1 & x_{N-1}\n",
    "        \\end{array}\n",
    "    \\right],\n",
    "$$\n",
    "\n",
    "where the constant in the first column of $M$ captures the zeropoint (i.e. the constant $y$-intercept) in the regression.\n",
    "\n",
    "The log-likelihood can then be written as\n",
    "\n",
    "$$ \\ln\\mathcal(L) = -\\frac{1}{2}(Y-M\\theta)^T C^{-1} (Y-M\\theta) - \\frac{1}{2}\\ln[\\mathrm{det}(C)]$$\n",
    "\n",
    "where we encapsulate uncertainties in the ($N\\times N$) covariance matrix\n",
    "\n",
    "$$C=\\left[\n",
    "        \\begin{array}{cccc}\n",
    "        \\sigma_{0}^2 & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "        0 & 0 & \\cdots & \\sigma_{N-1}^2 \\\\\n",
    "        \\end{array}\n",
    "    \\right].\n",
    "$$\n",
    "\n",
    "and the maximum likelihood solution for the regression can be analytically solved and expressed as\n",
    "\n",
    "$$\\theta = (M^T C^{-1} M)^{-1} (M^T C^{-1} Y),$$\n",
    "\n",
    "which minimizes the sum of squares, and gives uncertainties on $\\theta$ of \n",
    "\n",
    "$$\\Sigma_\\theta = \\left[\n",
    "                    \\begin{array}{cc}\n",
    "                    \\sigma_{\\theta_0}^2 & \\sigma_{\\theta_0\\theta_1} \\\\\n",
    "                    \\sigma_{\\theta_0\\theta_1} & \\sigma_{\\theta_1}^2\n",
    "                    \\end{array}\n",
    "                  \\right]\n",
    "                    = [M^T C^{-1} M]^{-1}.\n",
    "$$\n",
    "\n",
    "With `numpy` it is straightforward to write the matrices as multi-dimensional arrays, and do the linear algebra (provided the matrices are invertible), then calculate the regression coefficients.\n",
    "\n",
    "Let's start with some noisy data and the known solution. <font color='red'>Write the equation to predict the $y$ values and execute this cell.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta0 = 95 # Known value of b (theta0)\n",
    "theta1 = 10 # Known value of m (theta1)\n",
    "noise_std = 1.0 \n",
    "noise = np.random.randn(100, 1) * noise_std\n",
    "dy = noise\n",
    "\n",
    "X = np.random.rand(100, 1)\n",
    "y = theta0 + theta1 * X + dy\n",
    "\n",
    "X_grid = np.linspace(0,1,11)\n",
    "y_true = ____ + ____*____ # Write equation needed to plot a line based on known truth\n",
    "\n",
    "plt.plot(X, y, \"b.\", label='Measured')\n",
    "plt.plot(X_grid, y_true, \"k-\", label='Actual')\n",
    "plt.xlabel(\"$x$\", fontsize=14)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now solve for the $\\theta$ values. First using matrix math as shown below.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# diagonal noise matrix\n",
    "C = np.identity(len(X[:,0])) * noise_std**2\n",
    "\n",
    "# design matrix\n",
    "M = np.column_stack((np.ones(len(X[:,0])), X[:,0]))\n",
    "\n",
    "# Sigma^-1 = M^T * C^-1 * M\n",
    "A = np.dot(np.dot(M.T, np.linalg.pinv(C)), M)\n",
    "\n",
    "# M^T * C^-1 * y\n",
    "B = np.dot(np.dot(M.T, np.linalg.pinv(C)), y)\n",
    "\n",
    "# theta = Sigma * M^T * C^-1 * y\n",
    "theta = np.dot(np.linalg.pinv(A),B)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Use the results to make predictions and compare to the known answer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [1]]) #like X_grid, but just with the endpoints\n",
    "y_pred = ____ + ____*____  # Complete \n",
    "\n",
    "plt.plot(X_new, ____, \"r--\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X_grid, y_true, \"k-.\", label=\"Actual\") # Comment this out to see that it agrees with the above\n",
    "plt.plot(X, y, \"b.\", label='Measured')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "#N.B. you could also do this as \n",
    "#y_pred = np.dot(MM,theta)\n",
    "#if MM were defined appropriately\n",
    "#MM = np.column_stack((np.ones(len(X_grid)),X_grid))\n",
    "#y_pred = np.dot(MM,theta)\n",
    "#plt.plot(MM[:,1], y_pred, \"r--\", linewidth=2, label=\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now do it with [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from `Scikit-Learn`.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y, sample_weight=1.0)\n",
    "\n",
    "theta0 = lin_reg.intercept_\n",
    "theta1 = lin_reg.coef_\n",
    "\n",
    "print(theta0, theta1)\n",
    "y_pred2 = lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Print $C$, $M$, $A$, $B$, and $\\theta$ and make sure that you understand how these are constructed.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(C)\n",
    "print(M)\n",
    "print(A)\n",
    "print(B)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now compare `y_pred` to `y_pred2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, what we did was to replace\n",
    "```\n",
    "C = np.identity(len(X))*(dy*dy)\n",
    "M = np.column_stack((np.ones(len(X)),X))\n",
    "A = np.dot(np.dot(M.transpose(),np.linalg.pinv(C)),M)\n",
    "B = np.dot(np.dot(M.transpose(),np.linalg.pinv(C)),y)\n",
    "theta = np.dot(np.linalg.pinv(A),B)\n",
    "\n",
    "y_pred = np.dot(M_new,theta) # using design matrix at new x locations\n",
    "```\n",
    "\n",
    "with\n",
    "```\n",
    "LRmodel = LinearRegression()\n",
    "LRmodel.fit(X, y, sample_weight=noise_std)\n",
    "y_pred = LRmodel.predict(X_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is another example from the first panel of Figure 8.2, where we have done a straight-line regression to data with a non-linear correlation.  \n",
    "\n",
    "***These are redshifts and distance moduli for supernovae. Calibrating their relationship gives us the parameters that describe things like the expansion rate of the Universe (Hubble constant) and the fractional composition of baryons, dark energy, etc.***\n",
    "\n",
    "A reminder on nomenclature for `Scikit-Learn`: \n",
    "- $X$ is the multidimensional matrix of $N$ \"objects\", each with $K$ attributes.  \n",
    "- $y$ is the dependent variable that represents the measured value for each of those $N$ objects.  \n",
    "- What is new here is that are adding $dy$, which is the uncertainty on $y$. \n",
    "\n",
    "The fitting algorithms are going to be of the form:\n",
    "\n",
    "```model.fit(X,y,dy)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic, Figure 8.2, modified by GTR (to just plot first panel)\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from astropy.cosmology import LambdaCDM\n",
    "from astroML.datasets import generate_mu_z\n",
    "from astroML.linear_model import (LinearRegression, PolynomialRegression,\n",
    "                                  BasisFunctionRegression, NadarayaWatson)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "X = z_sample[:,None]\n",
    "y = mu_sample\n",
    "dy = dmu\n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "y_true = cosmo.distmod(z)\n",
    "mu_true = y_true\n",
    "\n",
    "X_grid = z[:,None]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our classifiers\n",
    "basis_mu = np.linspace(0, 2, 15)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "subplots = [221, 222, 223, 224]\n",
    "classifiers = [LinearRegression(),\n",
    "               PolynomialRegression(4),\n",
    "               BasisFunctionRegression('gaussian',\n",
    "                                       mu=basis_mu, \n",
    "                                       sigma=basis_sigma),\n",
    "               NadarayaWatson('gaussian', h=0.1)]\n",
    "text = ['Straight-line Regression',\n",
    "        '4th degree Polynomial\\n Regression',\n",
    "        'Gaussian Basis Function\\n Regression',\n",
    "        'Gaussian Kernel\\n Regression']\n",
    "\n",
    "# number of constraints of the model.  Because\n",
    "# Nadaraya-watson is just a weighted mean, it has only one constraint\n",
    "n_constraints = [2, 5, len(basis_mu) + 1, 1]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "fig.subplots_adjust(left=0.1, right=0.95,\n",
    "                    bottom=0.1, top=0.95,\n",
    "                    hspace=0.05, wspace=0.05)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "i=0 #Just show the straight-line regression\n",
    "\n",
    "# fit the data\n",
    "clf = classifiers[i]\n",
    "clf.fit(X, y, dy)\n",
    "\n",
    "mu_sample_fit = clf.predict(X)\n",
    "mu_fit = clf.predict(X_grid)\n",
    "mu_fit_lin = mu_fit #Save for later to compare\n",
    "\n",
    "chi2_dof = (np.sum(((mu_sample_fit - mu_sample) / dmu) ** 2)\n",
    "                / (len(mu_sample) - n_constraints[i]))\n",
    "\n",
    "ax.plot(z, mu_fit, '-r')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof,\n",
    "            ha='center', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.text(0.05, 0.95, text[i], ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the best-fit linear relationship is red, and the true generating relationship is dashed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Word of Caution\n",
    "\n",
    "[Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) is a demonstration of why you should always visualize your data and not just blindly use the parameters of some black-box fitting algorithm.\n",
    "\n",
    "![Anscombe's Quartet](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1000px-Anscombe%27s_quartet_3.svg.png)\n",
    "\n",
    "Each of these data sets has $11$ points and basic statistical properties that are identical. For example, they all have the same best fit when assuming a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate linear regression <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "In the above cases, we were doing 2-D linear regression with a univariate $X$.  If $X$ is instead multivariate, then we fit a hyperplane rather than a straight line\n",
    "\n",
    "$$y_i =\\theta_0 + \\theta_1x_{i1} + \\theta_2x_{i2} + \\cdots +\\theta_kx_{ik} + \\epsilon_i.$$\n",
    " \n",
    "The design matrix, $M$, is now \n",
    "\n",
    "$$M = \\left(\n",
    "        \\begin{array}{ccccccc}\n",
    "        1 & x_{01} & x_{02} & . & x_{0k}\\\\\n",
    "        1 & x_{11} & x_{12} & . & x_{1k}\\\\\n",
    "        . & . & . & .  & . \\\\\n",
    "        1 & x_{N1} & x_{N2} & . & x_{Nk}\\\\\n",
    "        \\end{array}\n",
    "      \\right)\n",
    "$$\n",
    "\n",
    "but the whole formalism is exactly the same as before.\n",
    "\n",
    "Scikit-Learn obviously has a [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) routine, but it does not explicitly account for heteroscedastic errors, so above we used the AstroML routine instead as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegression\n",
    "\n",
    "Xtest = np.random.random((100,2)) # 100 points in 2D\n",
    "dytest = np.random.random(100) # heteroscedastic errors\n",
    "ytest = np.random.normal(Xtest[:,0] + Xtest[:,1], dytest) # y = 0 + 1*x1 + 1*x2\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtest, ytest, dytest)\n",
    "\n",
    "print(model.coef_)\n",
    "#y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Polynomial Regression <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "We introduced regression with examples of straight-line fitting, but we can think of it more generically in terms of **[polynomical regression](https://en.wikipedia.org/wiki/Polynomial_regression)** with $y=f(x|\\theta)$ and\n",
    "\n",
    "$$y_i =\\theta_0 + \\theta_1 x_{i} + \\theta_2 x_{i}^2 + \\theta_3 x_{i}^3 + \\cdots.$$   \n",
    "\n",
    "For example, maybe $y$ depends on the area or volume of an object, but you have mesured the length. More generally, this is like fitting a model that is a Taylor expansion of the exact $f(x)$ to determine amplitudes of the different polynomial terms.\n",
    "\n",
    "For polynomical regression, the design matrix $M$ is now\n",
    " \n",
    "$$\n",
    "M = \\left(\n",
    "        \\begin{array}{cccccc}\n",
    "        1 & x_{0} & x_{0}^2 & x_{0}^3\\\\\n",
    "        1 & x_{1} & x_{1}^2 & x_{1}^3\\\\\n",
    "        . & . & . & . \\\\\n",
    "        1 & x_{N} & x_{N}^2 & x_{N}^3\\\\\n",
    "        \\end{array}\n",
    "    \\right).\n",
    "$$\n",
    "\n",
    "**NOTE:** Be careful with terminology!! ***This is still Linear Regression since the model only depends linearly on the parameters.*** The \"linear\" in linear regression corresponds to these parameters, not the independent variable $x$.\n",
    "\n",
    "As with linear regression, we'll use `PolynomialRegression` from `AstroML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Typical call\n",
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "Xtest = np.random.random((100,1))\n",
    "ytest = 2.5*Xtest[:,0]**2 + 5.0*Xtest[:,0]**3\n",
    "\n",
    "degree = 3\n",
    "model = PolynomialRegression(degree) # fit 3rd degree polynomial\n",
    "model.fit(Xtest, ytest)\n",
    "\n",
    "y_pred = model.predict(Xtest)\n",
    "n_constraints = degree + 1\n",
    "\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Recreate the supernovae figure from above now using the `PolynomialRegression` algorithm with `degree=4`. *(Hint: Don't overthink it.)* </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#fit data using standard package\n",
    "degree = ___\n",
    "n_constraints = degree + 1\n",
    "poly = ___(___)\n",
    "poly.fit(X, y, dy)\n",
    "mu_fit = poly.predict(X_grid)\n",
    "mu_sample_fit = poly.predict(X)\n",
    "\n",
    "chi2_dof = (np.sum(((mu_sample_fit - mu_sample) / dmu)**2) / \n",
    "            (len(mu_sample) - n_constraints))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "#plot the data\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(z, mu_fit, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, \n",
    "            fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof,\n",
    "    ha='center', va='bottom', \n",
    "        transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.text(0.05, 0.95, 'Polynomial regression', ha='left', va='top',\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "ax.plot(z, mu_fit_lin, '-r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best-fit $4$th degree polynomial is solid black, the linear fit from before is solid red, and the true generating relationship is dashed.\n",
    "\n",
    "<font color='red'>Can you make the same code do straight-line regression?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basis function regression <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "If we consider a function in terms of a sum over bases (this can be polynomials, Gaussians, sines) then we can solve for the linear amplitude coefficients using regression. ***So regression with arbitrary basis terms that may be non-linear in $x$ is still linear regression, since the model is linear in the regression parameters.*** \n",
    "\n",
    "Above we have used polynomials, but we could substitute $x_{0}^2$ etc for Gaussians (where we fix $\\sigma$ and $\\mu$ and fit for the amplitude) as long as the attribute we are fitting for is linear. So if straight-line regression is just a special case of polynomial regression, then polynomial regression is just a special case of basis function regression. All of these are linear regression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Basis function regression looks like this\n",
    "from astroML.linear_model import BasisFunctionRegression\n",
    "\n",
    "# data\n",
    "Xtest = np.random.random((100,1))\n",
    "ytest = np.random.normal(Xtest[:,0], dy)\n",
    "\n",
    "# mean positions of the 10 Gaussians in the model\n",
    "X_gridtest = np.linspace(0,1,10)[:, None]\n",
    "# widths of these Gaussians\n",
    "sigma_test = 0.1\n",
    "\n",
    "model = BasisFunctionRegression('gaussian', mu=X_gridtest, sigma=sigma_test)\n",
    "model.fit(Xtest, ytest, dytest)\n",
    "\n",
    "y_pred = model.predict(Xtest)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We'll now repeat the supernova data example using basis function regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Define our Gaussians\n",
    "nGaussians = 10\n",
    "basis_mu = np.linspace(0,2,nGaussians)[:, None]\n",
    "basis_sigma = 1.0 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "n_constraints = nGaussians+1\n",
    "\n",
    "#fit data using gaussian-based basis function regression\n",
    "bfr = BasisFunctionRegression('gaussian', mu=basis_mu, sigma=basis_sigma)\n",
    "bfr.fit(z_sample[:, None], mu_sample, dmu)\n",
    "mu_fit = bfr.predict(z[:, None])\n",
    "mu_sample_fit = bfr.predict(z_sample[:, None])\n",
    "chi2_dof = (np.sum(((mu_sample_fit - mu_sample) / dmu) ** 2) / (len(mu_sample) - n_constraints))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof,\n",
    "    ha='center', va='bottom', transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.text(0.05, 0.95, 'Basis Function regression', ha='left', va='top',\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "#ax.plot(z, mu_out, '-k', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the underlying Gaussians that are adding together in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Do it by hand so that we can overplot the Gaussians\n",
    "\n",
    "def gaussian_basis(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "M = np.zeros(shape=[nGaussians, z_sample.shape[0]])\n",
    "for i in range(nGaussians):\n",
    "    M[i] = gaussian_basis(z_sample, basis_mu[i], basis_sigma)\n",
    "\n",
    "M = np.matrix(M).T\n",
    "C = np.matrix(np.diagflat(dmu**2))\n",
    "Y = np.matrix(mu_sample).T\n",
    "coeff = (M.T * C.I * M).I * (M.T * C.I * Y)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0.1, right=0.95, bottom=0.1, \n",
    "                    top=0.95, hspace=0.05, wspace=0.05)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the gaussians and their sum\n",
    "i=0\n",
    "mu_fit = np.zeros(len(z))   \n",
    "for i in range(nGaussians):\n",
    "    mu_fit += coeff[i,0] * gaussian_basis(z, basis_mu[i], basis_sigma)\n",
    "    if (coeff[i,0] > 0.):\n",
    "        ax.plot(z,coeff[i,0] * gaussian_basis(z, basis_mu[i], basis_sigma), \n",
    "                color='C1')\n",
    "    else:\n",
    "        ax.plot(z,-coeff[i,0] * gaussian_basis(z, basis_mu[i], basis_sigma), \n",
    "                color='C1',ls='--')\n",
    "\n",
    "#plot the data\n",
    "ax.plot(z, mu_fit, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, \n",
    "            fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof,\n",
    "    ha='center', va='bottom', \n",
    "        transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(0.01, 48)\n",
    "ax.text(0.05, 0.95, 'Basis Function regression', ha='left', va='top',\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "#ax.plot(z, mu_out, '-k', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel Regression <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "In the case of Gaussian Basis Regression, if you think about it, we were back to the old problem of making a histogram.  Specifically, our Gaussians were evenly spaced over the range of interest. If we instead placed Gaussians at the location of every data point, we get Gaussian Kernel Regression instead. Or just **[Kernel Regression](https://en.wikipedia.org/wiki/Kernel_regression)** more generally since we don't *have* to have a Gaussian kernel function. It is also called **Nadaraya-Watson regression**.\n",
    "\n",
    "Given a kernel $K(x_i,x)$ (e.g., a Gaussian or top-hat) at each point we estimate the function value by\n",
    "\n",
    "$$f(x|K) = \\frac{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right) y_i}\n",
    "{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right)},$$\n",
    "\n",
    "which is a weighted sum of $y$ (weighted by distance) with\n",
    "\n",
    "$$w_i(x) = \\frac{ K\\left( \\frac{||x_i-x||}{h} \\right)}\n",
    "{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right)}.$$\n",
    "\n",
    "This **locally weighted regression** technique drives the regressed value to the nearest neighbor (when we have few points) which helps with extrapolation issues. As we saw with KDE, defining the correct bandwidth of the kernel is more important than the shape of the kernel itself and is done through **cross-validation**, which we'll talk about next time.\n",
    "\n",
    "Nadaraya-Watson is implemented in `AstroML` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astroML.linear_model import NadarayaWatson\n",
    "\n",
    "Xtest = np.random.random((100,2))\n",
    "ytest = Xtest[:,0] + Xtest[:,1]\n",
    "\n",
    "model = NadarayaWatson(kernel='gaussian', h=0.05)\n",
    "model.fit(Xtest,ytest)\n",
    "\n",
    "y_pred = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using Nadaraya-Watson on our supernova data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "from astroML.datasets import generate_mu_z\n",
    "from astroML.linear_model import NadarayaWatson\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate data\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "from astropy.cosmology import LambdaCDM\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "n_constraints = 1\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "#fit data using standard package\n",
    "nwreg = NadarayaWatson('gaussian', 0.05)\n",
    "nwreg.fit(z_sample[:, None], mu_sample)\n",
    "mu_sample_fit = nwreg.predict(z_sample[:, None])\n",
    "mu_fit = nwreg.predict(z[:, None])\n",
    "chi2_dof = (np.sum(((mu_sample_fit - mu_sample) / dmu) ** 2) / \n",
    "            (len(mu_sample) - n_constraints))\n",
    "\n",
    "\n",
    "#plot the data\n",
    "ax.plot(z, mu_fit, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.text(0.5, 0.05, r\"$\\chi^2_{\\rm dof} = %.2f$\" % chi2_dof,\n",
    "    ha='center', va='bottom', transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "ax.text(0.05, 0.95, 'Nadaraya-Watson', ha='left', va='top',\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "#ax.plot(z, mu_out, '-k', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Over/Under-fitting <a class=\"anchor\" id=\"seven\"></a>\n",
    "\n",
    "We already talked a little bit about overfitting, but let's dive down deeper now that we are trying to fit complicated models. We'll use a 1-D model with homoscedastic errors for the sake of illustration, but this discussion applies to more complicated data as well.\n",
    "\n",
    "To be clear, our data consists of $X_{\\rm train}$, $y_{\\rm train}$, and $X_{\\rm test}$ and we are trying to predict $y_{\\rm test}$.  \n",
    "\n",
    "Let's take an example where\n",
    "\n",
    "$$0\\le x_i \\le 3$$\n",
    "\n",
    "and\n",
    "\n",
    "$$y_i = x_i \\sin(x_i) + \\epsilon_i,$$\n",
    "\n",
    "where the noise, $\\epsilon_i$ is given by $\\mathscr{N}(0,0.1)$.\n",
    "\n",
    "In the example below, we draw 20 evenly spaced points from this distribution and fit them with a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.12, plot made bigger by SRT and GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# First figure: plot points with a linear fit\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x, y, marker='x', c='k', s=30)\n",
    "\n",
    "p = np.polyfit(x, y, 1)\n",
    "y_fit = np.polyval(p, x_fit)\n",
    "\n",
    "ax.text(0.03, 0.96, \"d = 1\", transform=plt.gca().transAxes,\n",
    "        ha='left', va='top',\n",
    "        bbox=dict(ec='k', fc='w'))\n",
    "\n",
    "ax.plot(x_fit, y_fit, '-b')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This model **underfits** the data and is said to be \"biased\" (in the sense that the estimated model parameters deviate significantly from the true model parameters).  \n",
    "- A straight line is a polynomial of order 1, so let's try polynomials of higher order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.13, panels made bigger by GTR and sizing altered by SRT\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "from matplotlib import ticker\n",
    "from matplotlib.patches import FancyArrow\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Second figure: plot fit for several orders of polynomial\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "fig.subplots_adjust(wspace=0.03, bottom=0.15,\n",
    "                    top=0.95, left=0.07, right=0.97)\n",
    "\n",
    "for i, d in enumerate([2, 3, 19]):  #Try 2nd, 3rd and 19th order\n",
    "    ax = fig.add_subplot(131 + i)\n",
    "    ax.scatter(x, y, marker='x', c='k', s=30)\n",
    "\n",
    "    p = np.polyfit(x, y, d)\n",
    "    y_fit = np.polyval(p, x_fit)\n",
    "\n",
    "    ax.plot(x_fit, y_fit, '-b')\n",
    "    ax.set_ylim(-0.1, 2.1)\n",
    "    ax.set_xlim(-0.2, 3.2)\n",
    "    if i in (1, 2):\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax.set_ylabel('$y$')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.text(0.08, 0.94, \"d = %i\" % d, transform=ax.transAxes,\n",
    "            ha='left', va='top',\n",
    "            bbox=dict(ec='k', fc='w'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, $2$, $3$, and all the way up to $19$. We see that $3$ is pretty good in that it seems to be relatively unbiased and also lacks the high variance of the `degree=19` fit (which is also unbiased, but overfit). ***So, how do we choose the \"right\" answer?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation <a class=\"anchor\" id=\"eight\"></a>\n",
    "\n",
    "More regression coefficients improve the ability of the model to fit all the points (reduced **bias**), but at the expense of model complexity and **variance**. *Of course* we can fit a Nth-degree polynomial to N data points, but that would be foolish. We'll determine the best trade-off between bias and variance through [cross-validation](https://en.wikipedia.org/wiki/Cross-validation).\n",
    "\n",
    "When we increase the complexity of a model, the data points fit the model more and more closely. However, this process does not necessarily result in a better fit to the data. Rather, if the degree is too high, then we are ***overfitting*** the data. The model has high variance, meaning that a small change in a training point can change the model dramatically.  \n",
    "\n",
    "We can evaluate this using a **training set** ($50-70\\%$ of sample), a **cross-validation set** ($15-25\\%$) and a **test set** ($15-25\\%$).\n",
    "\n",
    "The training set is used the determine the model paramters, $\\theta_j$.  The training data and cross-validation data then are both used to evaluate the training and cross-validation rms errors ($\\epsilon_{\\rm tr}$ and $\\epsilon_{\\rm CV}$; evaluated here for polynomial regression):\n",
    "\n",
    "$$\\epsilon_{\\rm cv/tr} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{N_{\\rm cv/tr}}\n",
    "  \\left[y_i - \\sum_{m=0}^d \\theta_0^{(n)}x_i^m\\right]}$$\n",
    "\n",
    "> *Why do we need both a training set and a cross-validation set?* \n",
    "> - The **model parameters, $\\theta_j$, are learned from the training set**,\n",
    "> - But the **\"hyperparameters\" (in this case the model degree) are learned from the cross-validation set**. \n",
    "\n",
    "> *The test set then provides the best estimate of the error expected for a new set of unlabeled data.*\n",
    "\n",
    "We show this graphically in the next figure (Ivezic, 8.14), where the **training and cross-validation rms errors are computed as a function of polynomial degree**, and also compared with the **model AIC and BIC**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.14, panels made bigger by GTR\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Third figure: plot errors as a function of polynomial degree d\n",
    "d = np.arange(0, 21)\n",
    "training_err = np.zeros(d.shape)\n",
    "crossval_err = np.zeros(d.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(len(d)):\n",
    "    p = np.polyfit(x, y, d[i])\n",
    "    training_err[i] = np.sqrt(np.sum((np.polyval(p, x) - y) ** 2)\n",
    "                              / len(y))\n",
    "    crossval_err[i] = np.sqrt(np.sum((np.polyval(p, x_cv) - y_cv) ** 2)\n",
    "                              / len(y_cv))\n",
    "\n",
    "BIC_train = np.sqrt(len(y)) * training_err / dy + d * np.log(len(y))\n",
    "BIC_crossval = np.sqrt(len(y)) * crossval_err / dy + d * np.log(len(y))\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(d, crossval_err, '--k', label='cross-validation')\n",
    "ax.plot(d, training_err, '-k', label='training')\n",
    "ax.plot(d, 0.1 * np.ones(d.shape), ':k')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 0.8)\n",
    "\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('rms error')\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(d, BIC_crossval, '--k', label='cross-validation')\n",
    "ax.plot(d, BIC_train, '-k', label='training')\n",
    "\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('polynomial degree')\n",
    "ax.set_ylabel('BIC')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For low order, both the training and CV error are high. This is sign of a **high-bias model that is underfitting** the data.  \n",
    "- For high order, the training error becomes small (by definition), but the CV error is large. This is the sign of a **high-variance model that is overfitting** the data. It is matching the subtle fluctuations in the training data that aren't really real, and this shows up in the CV analysis.\n",
    "- The AIC and BIC give similar results.\n",
    "\n",
    "Hopefully that helps you understand how to use cross validation to help you both **fit your model and decide on the optimal level of model complexity**, but maybe it doesn't help you apply it to your own data.  \n",
    "\n",
    "However, we've already seen how to do this with **[GridSearchCV](https://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV)**, where for this case, we'd just be varying one parameter (even though `GridSearchCV` can vary many)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Curves\n",
    "\n",
    "We can use a tool called a **[learning curve](https://en.wikipedia.org/wiki/Learning_curve)** to determine if (for a given model) having more training data would help improve the model fitting. This is a different question than above-- rather than try to get a better model of the data, we're trying to improve the quality of our data set. \n",
    "\n",
    "The training and CV error are computed as a function of the number of training points. In general:\n",
    "- ***The training error increases with $N_\\mathrm{train}$.*** For a given model, it's easier to fit fewer data points.\n",
    "- ***The CV error decreases wtih $N_\\mathrm{train}$.*** For a given model, a greater number of training points reduces the chances of over-fitting, resulting in better performance of the model in the cross-validation stage.\n",
    "\n",
    "Let's look at this for the same data and model as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Ivezic v2, Figure 8.15, panels made bigger by GTR and starting at 2 points.\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our functional form\n",
    "def func(x, dy=0.1):\n",
    "    return np.random.normal(np.sin(x) * x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# select the (noisy) data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 3, 22)[1:-1]\n",
    "dy = 0.1\n",
    "y = func(x, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Select the cross-validation points\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(20)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "x_fit = np.linspace(0, 3, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Fourth figure: plot errors as a function of training set size\n",
    "np.random.seed(0)\n",
    "x = 3 * np.random.random(100)\n",
    "y = func(x)\n",
    "\n",
    "np.random.seed(1)\n",
    "x_cv = 3 * np.random.random(100)\n",
    "y_cv = func(x_cv)\n",
    "\n",
    "Nrange = np.arange(2, 101, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(left=0.15, top=0.95)\n",
    "\n",
    "for subplot, d in zip([211, 212], [2, 3]):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "    training_err = np.zeros(Nrange.shape)\n",
    "    crossval_err = np.zeros(Nrange.shape)\n",
    "\n",
    "    for j, N in enumerate(Nrange):\n",
    "        p = np.polyfit(x[:N], y[:N], d)\n",
    "        training_err[j] = np.sqrt(np.sum((np.polyval(p, x[:N])\n",
    "                                          - y[:N]) ** 2) / len(y))\n",
    "        crossval_err[j] = np.sqrt(np.sum((np.polyval(p, x_cv)\n",
    "                                          - y_cv) ** 2) / len(y_cv))\n",
    "\n",
    "    ax.plot(Nrange, crossval_err, '--k', label='cross-validation')\n",
    "    ax.plot(Nrange, training_err, '-k', label='training')\n",
    "    ax.plot(Nrange, 0.1 * np.ones(Nrange.shape), ':k')\n",
    "    ax.legend(loc=1)\n",
    "    ax.text(0.03, 0.94, \"d = %i\" % d, transform=ax.transAxes,\n",
    "            ha='left', va='top', bbox=dict(ec='k', fc='w'))\n",
    "\n",
    "    ax.set_ylim(0, 0.4)\n",
    "\n",
    "    ax.set_xlabel('Number of training points')\n",
    "    ax.set_ylabel('rms error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see two regimes:\n",
    "\n",
    "1. ***The training and CV errors have converged.*** This indicates that the model is dominated by bias. Increasing the number of training points is futile. If the error is too high, you instead need a more complex model, not more training data.\n",
    "2. ***The training error is smaller than the CV error.*** This indicates that the model is dominated by variance.  Increasing the number of training points may help to improve the model.\n",
    "\n",
    "In both cases, for small numbers of training points, the difference between the training and CV errors indicates that more data well help.  \n",
    "\n",
    "For the top plot, the convergence of the training and CV errors indicates that adding further data will not reduce the error as it is dominated by ***bias***. A more sophisticated model is needed. The training error starts at zero because we are predicting the training data perfectly, but the CV error is high, indicating that the training data aren't sufficiently representative.\n",
    "\n",
    "Again, hopefully that helps you understand what learning curves tell you, but maybe it doesn't help you apply it to your own data.  So, let's see how sklearn does it using [sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve).  \n",
    "\n",
    "Below we apply this to the [Boston Housing data](http://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset). This data set contains 12 attributes that can be used to predict the price of houses in Boston. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Execute this cell to read in the data\n",
    "#Also identify the index of the \"Number of Rooms\" attribute\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xscaled = scaler.fit_transform(X)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "# Ten training sample sizes from 10% to 100%\n",
    "train_sizes, train_scores_linreg, test_scores_linreg = \\\n",
    "    learning_curve(lin_reg, Xscaled, y, train_sizes=np.linspace(0.1, 1, 10), \\\n",
    "    scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.plot(train_sizes, -test_scores_linreg.mean(1), 'o-', color=\"r\", label=\"Val\")\n",
    "plt.plot(train_sizes, -train_scores_linreg.mean(1), 'o-', color=\"g\", label=\"Train\")\n",
    "                   \n",
    "plt.xlabel(\"Train size\",fontsize=14)\n",
    "plt.ylabel(\"Mean Squared Error\",fontsize=14)\n",
    "plt.title('Learning curves',fontsize=14)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylim(0,200)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, we barely have enough data.  This is important to know when we are deciding on the number of cross validations.  \n",
    "\n",
    "Once you are happy with the size of your training set and your choice of model, and now just want to get the best model parameters using *all* of the data (not just whatever fraction is in your training set), then we can use **[cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)**.\n",
    "\n",
    "Let's use this to predict the price of houses in Boston where each house gets to be part of one of the test sets by doing a $10$-fold cross validation. We will then plot the *prediction* versus the *actual* value of $y$, which should lie along the line $y=y_\\mathrm{pred}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "scaler = StandardScaler() #Complete\n",
    "Xscaled = scaler.fit_transform(X) #Complete\n",
    "linreg = LinearRegression() #Complete\n",
    "linreg.fit(Xscaled, y) #Complete\n",
    "\n",
    "# Do a 10-fold cross validation\n",
    "# Then try a 3-fold cross validation\n",
    "# Can you understand the difference?\n",
    "ypred = cross_val_predict(linreg, Xscaled, y, cv=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.scatter(y, ypred, edgecolors=(0, 0, 0))\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Actual [x1000]',fontsize=14)\n",
    "ax.set_ylabel('Predicted [x1000]',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now try a 3-fold cross validation. Can you understand the difference?</font>\n",
    "\n",
    "As a final recap, in case you're still not sure about $k$-fold CV, the procedure is thus:\n",
    "\n",
    "- Split the data into $k+1$ subsets; the test set and $k$ CV sets. How you do this is up to you, but typically through random shufflings with equal numbers of points.\n",
    "- $k$ models are trained, each time leaving out one of the CV sets in order to measure the CV error.\n",
    "- The final training and CV error can be computed using the mean or median of the set of results. The median can be more reliable.\n",
    "\n",
    "See the following GIF of $3$-fold CV from the [Wikipedia article](https://www.wikiwand.com/en/Cross-validation_(statistics)).\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/KfoldCV.gif/1920px-KfoldCV.gif?1616979144969)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:astr8070] *",
   "language": "python",
   "name": "conda-env-astr8070-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
