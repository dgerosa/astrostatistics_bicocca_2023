{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probability & Statistics: II\n",
    "\n",
    "*Davide Gerosa (Milano-Bicocca)*\n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Ivezic textbook](https://press.princeton.edu/books/hardcover/9780691198309/statistics-data-mining-and-machine-learning-in-astronomy) Chapter 3. \n",
    "\n",
    "\n",
    "This course is based on previous work by many people. See [here]((https://github.com/dgerosa/astrostatistics_bicocca_2023/blob/main/README.md) for credits.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pylab as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo integration\n",
    "\n",
    "You might have seen this already, but Monte Carlo integration is an absolute crucial notion in modern statistics (modern because it relies on generating a large number of data points, which has been made possible by computers). Suppose you have very a complicated integral to solve and that you can write down the integrand as a product of $f(x)$ and $p(x)$ with $\\int p(x)=1$ (this is not restrictive at all! See below). My nasty integral is\n",
    "\n",
    "$$\\int f(x) p(x) dx$$\n",
    "\n",
    "\n",
    "Provided one can **evaluate** $f(x)$ and **sample** $p(x)$, then\n",
    "\n",
    "\n",
    "$$\\int f(x) p(x) dx \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i) $$\n",
    "\n",
    "where $x_i$ are samples drawn from $p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example.\n",
    "\n",
    "Ok try to integrate this crazy thing. Pen and paper? Good luck.\n",
    "    \n",
    "$$\\int_{0.3}^4 dx \\frac{\\exp(x) \\sqrt{4x+3}  \\log(x^6)}{\\tanh(x)}$$\n",
    "\n",
    "Pen and paper? Good luck. With a computer? Easy peasy!\n",
    "\n",
    "First, I divide and multiply by $4-0.3$. This is because (see below) 1/(4-0.3) is the uniform distribution $p(x)$ for x between 0.3 and 4.\n",
    "\n",
    "$$ (4-0.3) \\times \\int_{0.3}^{4} \\frac{dx}{(4-0.3)} \\frac{\\exp(x) \\sqrt{4x+3} * \\log(x^6)}{\\tanh(x)}$$\n",
    "\n",
    "Now I have $p(x)=\\frac{1}{4-0.3}$ and $f(x) = \\frac{\\exp(x) \\sqrt{4x+3} * \\log(x^6)}{\\tanh(x)}$. All I need to do is generate numbers from $p$, put them into $f$, and sum the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax= 4\n",
    "xmin=0.3\n",
    "\n",
    "xi=np.random.uniform(xmin,xmax, 100000)\n",
    "\n",
    "def fun(x):\n",
    "    return np.exp(x)*(4*x+3)**0.5 * np.log(x**6)/np.tanh(x)\n",
    "\n",
    "integral = (xmax-xmin) * np.mean(fun(xi))\n",
    "\n",
    "integral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result:\n",
    "https://www.wolframalpha.com/input/?i=integrate+from+0.3+to+4+exp%28x%29+sqrt%284x%2B3%29+*+ln%28x%5E6%29+%2Ftanh%28x%29\n",
    "\n",
    "Run the cell again. And again.  We'll understand the errors in a bit.\n",
    "\n",
    "#### Always remember Monte Carlo integration, it's extremely useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics \n",
    "\n",
    "As we've said, our goal is to estimate $h(x)$ given some measured data, allowing us to reconstruct the data-based distribution $f(x)$. An arbitrary distribution can be characterized by **location** parameters (i.e., position), **scale** parameters (i.e., width), and **shape** parameters. These parameters are called ***descriptive statistics***.\n",
    "\n",
    "The distribution we're trying to characterize could be anything, e.g. (from my field) the distribution of masses of binary black-hole systems as discovered by gravitational-wave detectors. We really don't know the answer to this well, and the problem is made more complicated by things like detector selection effects (heavier systems are more likely to be observed), and blurring effects from measurement precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to the distribution we've seen already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Based on Ivezic v2, Figure 6.8; edited by G. T. Richards, S. R. Taylor, and D. Gerosa\n",
    "\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from astropy.visualization import hist\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=14, usetex=True)\n",
    "%config InlineBackend.figure_format='retina' # very useful command for high-res images\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate our data: a mix of several Cauchy distributions\n",
    "# In reality nature generates data for you\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "N = 2000 # number of data points\n",
    "mu_gamma_f = [(5, 1.0, 0.1),\n",
    "              (7, 0.5, 0.5),\n",
    "              (9, 0.1, 0.1),\n",
    "              (12, 0.5, 0.2),\n",
    "              (14, 1.0, 0.1)]\n",
    "hx = lambda x: sum([f * stats.cauchy(mu, gamma).pdf(x)\n",
    "                          for (mu, gamma, f) in mu_gamma_f])\n",
    "x = np.concatenate([stats.cauchy(mu, gamma).rvs(int(f * N), random_state=random_state)\n",
    "                    for (mu, gamma, f) in mu_gamma_f])\n",
    "random_state.shuffle(x)\n",
    "x = x[x > -10]\n",
    "x = x[x < 30]\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the results\n",
    "fig,ax = plt.subplots(figsize=(10, 10))\n",
    "xgrid = np.linspace(-10, 30, 1000)\n",
    "\n",
    "# True distribution: you typically don't have it! Only have the samples!\n",
    "if False:\n",
    "    ax.plot(xgrid, hx(xgrid), ':', color='black', zorder=3,\n",
    "            label=\"$h(x)$, Generating Distribution\")\n",
    "\n",
    "# A simple histogram\n",
    "# But try changing the number of bins!\n",
    "if True:\n",
    "    ax.hist(x,density=True,color='C0',bins=100,histtype='step',lw=2, label='Histogram, 100 bins')\n",
    "\n",
    "# Something more sophisticated: Kernel Density Estimation\n",
    "# But try changing the bandwith! \n",
    "if False:\n",
    "    kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n",
    "    kde.fit(x[:, None])\n",
    "    dens_kde = np.exp(kde.score_samples(xgrid[:, None]))\n",
    "    ax.plot(xgrid, dens_kde, '-', color='C1', zorder=3,\n",
    "            label=\"$f(x)$, non-parametric (KDE)\")\n",
    "\n",
    "# Use Gaussian Mixtures with a pre-defined number of clusters (13)\n",
    "if False:\n",
    "    gmm = GaussianMixture(n_components=13).fit(x.reshape(-1, 1))\n",
    "    logprob = gmm.score_samples(xgrid.reshape(-1, 1))\n",
    "    fx = lambda j : np.exp(gmm.score_samples(j.reshape(-1, 1)))\n",
    "    ax.plot(xgrid, fx(np.array(xgrid)), '-', color='C2',\n",
    "            label=\"$f(x)$, parametric (13 Gaussians)\")\n",
    "\n",
    "# Plot cosmetics\n",
    "ax.text(0.02, 0.95, \"%i points\" % N, ha='left', va='top',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "ax.set_ylabel('$p(x)$',fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "ax.set_xlabel('$x$',fontsize=14)\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(-0.01, 0.4001)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know that the **mean** of a sample is \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i$$ \n",
    "\n",
    "This is actually known as the **sample arithmetic mean**, and derives from *Monte Carlo integration* to get the first moment of the distribution, i.e. \n",
    "\n",
    "$$\\mu = E(x) = \\langle x \\rangle = \\int_{-\\infty}^{\\infty} x h(x)\\,dx \\approx \\frac{1}{N}\\sum_{i=1}^N x_i $$\n",
    "\n",
    "where $\\{x_i\\}$ are random samples from the properly normalized $h(x)$, and $E(\\cdot)$ means the **expectation value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the mean is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =x.copy() #Why am I doing this?\n",
    "\n",
    "mean = np.mean(data)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's most common to compute the mean, it may surprise you to learn that some distributions do not have formally calculable means (integration gives infinity). In these and other cases, the **median** is a more *robust* estimator of the (true) mean location of the distribution.  That's because it is less affected by **outliers**.\n",
    "\n",
    "To understand the previous statement, think about multiplying all numbers above the 50th percentile (i.e. the median) by 100, or even just replacing them with larger numbers. The mean would be strongly affected by these corrupted points, but **cumulative statistics based on the ordering of samples would remain unaffected by the outlier corruption**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(data)\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to unpack what is happening here\n",
    "median = np.median(data)\n",
    "\n",
    "mask = data > 15\n",
    "data2 = data.copy()\n",
    "data2[mask] = 10000\n",
    "\n",
    "newmedian = np.median(data2)\n",
    "newmean = np.mean(data2)\n",
    "\n",
    "print(median, newmedian)\n",
    "print(mean, newmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other descriptive statistics are related to higher order moments of the distribution. Beyond the \"average\" *location* value, we'd like to know something about **deviations** from the average (which is related to the *shape* of the distribution).  The simplest thing to compute is $$d_i = x_i - \\mu.$$  However, the average deviation is zero by definition of the mean.  The next simplest thing to do is to compute the **mean absolute deviation (MAD)**:\n",
    "\n",
    "$$\\frac{1}{N}\\sum|x_i-\\mu|,$$\n",
    "\n",
    "but the absolute values can hide the true scatter of the distribution [(example)](http://www.mathsisfun.com/data/standard-deviation.html). But this is used, see regularization schemes further on in the class. So the next simplest thing to do is to square the differences $$\\sigma^2 = \\frac{1}{N}\\sum(x_i-\\mu)^2,$$ which we call the **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *variance* $V$ is the just expectation value of $(x-\\mu)^2$ (and related to the 2nd moment)\n",
    "\n",
    "$$\\sigma^2 = V = E((x-\\mu)^2)\\int_{-\\infty}^{\\infty}  (x-\\mu)^2 h(x) dx,$$\n",
    "\n",
    "where $\\sigma$ is the **standard deviation**. Again, the integral gets replaced by a sum for discrete distributions. While most familiar for Gaussian distributions, you can compute the variance even if your distribution is not Gaussian.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.var(data)\n",
    "std = np.std(data)\n",
    "print(var, std)\n",
    "print(np.isclose(std**2,var)) #Why am I not doing std**2==var?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the **Median Absolute Deviation (also MAD)** given by\n",
    "\n",
    "$${\\rm median} (|x_i-{\\rm median}(\\{x_i\\})|)$$\n",
    "\n",
    "where $\\sigma = 1.4826\\,{\\rm MAD}$ for a Gaussian distribution (but note that we aren't using a Gaussian distribution above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import median_absolute_deviation\n",
    "MAD = median_absolute_deviation(data)\n",
    "print(MAD,MAD*1.4826)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$P\\%$ quantiles (or the $p^\\mathrm{th}$ percentile, $q_p$)** are computed as\n",
    "$$\\frac{p}{100} = H(q_p) = \\int_{-\\infty}^{q_p}h(x) dx$$\n",
    "\n",
    "The full integral from $-\\infty$ to $\\infty$ is 1 (100%).  So, here you are looking for the value of x that accounts for $p$ percent of the distribution.\n",
    "\n",
    "For example, the 25th, 50th, and 75th percentiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "q25, q50, q75 = np.percentile(data, [25, 50, 75])\n",
    "print(q25, q50, q75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data,bins=100,histtype='step');\n",
    "\n",
    "plt.axvline(np.percentile(data,50),c='red')\n",
    "plt.axvline(np.percentile(data,5),c='red',ls='dotted')\n",
    "plt.axvline(np.percentile(data,95),c='red',ls='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region between the 5th and the 95th percentile contains 90% of the samples. In Bayesian statistics, this is the 90% credible interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **interquartile range** is the difference between the 25th and 75th percentiles, $q_{75} - q_{25}$.\n",
    "\n",
    "Just as with the median, the interquartile range is a more *robust* estimator of the scale of a distribution than the standard deviation.  So, one can create a standard-deviation-esque measurement (at least for a Gaussian) from the interquartile range as\n",
    "\n",
    "$$\\sigma_G = 0.7413\\times(q_{75} - q_{25})$$  \n",
    "\n",
    "The normalization makes it *unbiased* for a perfect Gaussian (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell. Think about and discuss the results.\n",
    "from astroML import stats as astroMLstats\n",
    "\n",
    "# original data\n",
    "print(astroMLstats.sigmaG(data), np.std(data))\n",
    "\n",
    "# corrupted by outliers\n",
    "print(astroMLstats.sigmaG(data2), np.std(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell. Cumulative statistics take longer to compute, but are more robust.\n",
    "%timeit np.mean(data), np.std(data)\n",
    "%timeit np.median(data), astroMLstats.sigmaG(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **mode** is the most probable value, determined from the peak of the distribution, which is the value where the derivative is 0 (i.e. the turning point):\n",
    "\n",
    "$$ \\left(\\frac{dh(x)}{dx}\\right)_{x_m} = 0$$\n",
    "\n",
    "A good approximation for the mode is (the distribution needs to be somewhat close to Gaussian, for a proof: [Lupton 1993](https://press.princeton.edu/books/hardcover/9780691074290/statistics-in-theory-and-practice))\n",
    "\n",
    "$$x_m = 3q_{50} - 2\\mu$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "mode = 3*q50 - 2*mean\n",
    "print(mode, mean, median)\n",
    "\n",
    "# Note: don't rely on scipy.stats.mode()!!\n",
    "# It gives the most common value of an array, \n",
    "# but we have a random sample of unique draws\n",
    "print(scipy.stats.mode(data), \"This is non sense!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful ***shape*** measures include the \"higher order\" moments (the **skewness** and **kurtosis**):\n",
    "\n",
    "$$\\mathbf{Skewness}\\quad\\quad \\Sigma = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^3 h(x) dx,$$\n",
    " \n",
    "$$\\mathbf{Kurtosis}\\quad\\quad K = \\int_{-\\infty}^{\\infty}  \\left(\\frac{x-\\mu}{\\sigma}\\right)^4 h(x) dx  - 3.$$\n",
    "\n",
    "The skewness measures the distribution's *asymmetry*. Distribution's with long tails to larger $x$ values have positive $\\Sigma$. \n",
    "\n",
    "The kurtosis measures how peaked or flat-topped a distribution is, with strongly peaked ones being positive and flat-topped ones being negative. $K$ is calibrated to a Gaussian distribution (hence the subtraction of $3$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://www.astroml.org/_images/fig_kurtosis_skew_1.png](https://www.astroml.org/_images/fig_kurtosis_skew_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = scipy.stats.skew(data)\n",
    "kurt = scipy.stats.kurtosis(data)\n",
    "print(skew, kurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary descriptive statistics for our distribution\n",
    "print(\"Location: \", mean, median, mode)\n",
    "print(\"Scale: \", var, std, astroMLstats.sigmaG(data))\n",
    "print(\"Shape: \", skew, kurt)\n",
    "print(\"Some percentiles: \", q25, q50, q75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample versus Population statistics <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Statistics estimated from the *data* are called **sample statistics** as compared to **population statistics** derived from knowing the functional form of the pdf.\n",
    "\n",
    "Specifically, $\\mu$ is the **population mean**, i.e., it is the expectation value of $x$ for $h(x)$.  But we don't *know* $h(x)$.  So the **sample mean**, $\\overline{x}$, is an ***estimator*** of $\\mu$, defined as\n",
    "\n",
    "$$\\overline{x} \\equiv \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "\n",
    "which we determine from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the **population variance** $\\sigma^2$, we have the **sample variance**, $s^2$, where\n",
    "\n",
    "$$s^2 = \\frac{1}{N-1}\\sum_{i=1}^N(x_i-\\overline{x})^2$$\n",
    "\n",
    "The $N-1$ denominator (instead of $N$) accounts for the fact that we determine $\\overline{x}$ from the data itself instead of using a known $\\mu$. This is called [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).\n",
    "\n",
    "Ideally one tries to work in a regime where $N$ is large enough that we can be lazy and ignore this. \n",
    "\n",
    "So the mean and variance of a distribution are $\\mu$ and $\\sigma^2$.  The *estimators* of the distribution are $\\overline{x}$ (or $\\hat{x}$) and $s^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pill of modern astrophysics from my research.** We computed Bessel's correction in the context of Hierarchical Bayesian statistics for gravitational-wave events in a recent paper. [Moore and Gerosa (2021), \"Population-informed priors in gravitational-wave astronomy\"](https://arxiv.org/abs/2108.02462). It's a fun paper to read if you're interested in the stats+gravity interplay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uncertainty of sample statistics\n",
    "\n",
    "We would also like to know the uncertainty of our estimates $\\overline{x}$ and $s$. \n",
    "\n",
    "Note that $s$ is the width estimate of the underlying distribution; it is **NOT** the uncertainty of $\\overline{x}$. This is a common misconception.\n",
    "\n",
    "Rather the uncertainty of $\\overline{x}$, $\\sigma_{\\overline{x}}$ is \n",
    "\n",
    "$$ \\sigma_{\\overline{x}} = \\frac{s}{\\sqrt{N}},$$\n",
    "\n",
    "which we call the **standard error of the mean**. The uncertainty of $s$ itself is\n",
    "\n",
    "$$\\sigma_s = \\frac{s}{\\sqrt{2(N-1)}} = \\frac{1}{\\sqrt{2}}\\sqrt{\\frac{N}{N-1}}\\sigma_{\\overline{x}}.$$\n",
    "\n",
    "Note that for large $N$, $\\sigma_{\\overline{x}} \\sim \\sqrt{2}\\sigma_s$ and for small $N$, $\\sigma_s$ is not much smaller than $s$.\n",
    "\n",
    "Another useful uncertainty estimate is for computing quantiles. The **standard error of a quantile** is\n",
    "\n",
    "$$ \\sigma_{q_p} = \\frac{1}{h_p}\\sqrt{\\frac{p(1-p)}{N}}$$\n",
    "\n",
    "where $p$ is between $0$ and $1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Try this at home***. \n",
    "- Generate some data\n",
    "- Compute $\\bar x$ and $s$\n",
    "- Do it many times\n",
    "- Estimate the errors\n",
    "- Compare with the expressions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Univariate distributions  (and an intro to scipy.stats)\n",
    "\n",
    "If we are attempting to characterize our data in a way that is **parameterized**, then we need a functional form for a **distribution**.  There are many naturally occurring distributions.  The book goes through quite a few of them.  Here we'll just talk about a few basic ones to get us started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Uniform Distribution\n",
    "\n",
    "The uniform distribution is perhaps more commonly called a \"top-hat\" or a \"box\" distribution.  It is specified by a mean, $\\mu$, and a width, $W$, where\n",
    "\n",
    "$$p(x|\\mu,W) = \\frac{1}{W}$$\n",
    "\n",
    "over the range $|x-\\mu|\\le \\frac{W}{2}$ and $0$ otherwise.  That says that \"given $\\mu$ AND $W$, the probability of $x$ is $\\frac{1}{W}$\" (as long as we are within a certain range).\n",
    "\n",
    "Since we are used to thinking of a Gaussian as the *only* type of distribution the concept of $\\sigma$ (aside from the width) may seem strange.  But $\\sigma$ as mathematically defined above applies here and\n",
    "$$\\sigma = \\frac{W}{\\sqrt{12}}.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of a uniform distribution\n",
    "---------------------------------\n",
    "Figure 3.7.\n",
    "This shows an example of a uniform distribution with various parameters.\n",
    "We'll generate the distribution using::\n",
    "    dist = scipy.stats.uniform(...)\n",
    "Where ... should be filled in with the desired distribution parameters\n",
    "Once we have defined the distribution parameters in this way, these\n",
    "distribution objects have many useful methods; for example:\n",
    "* ``dist.pmf(x)`` computes the Probability Mass Function at values ``x``\n",
    "  in the case of discrete distributions\n",
    "* ``dist.pdf(x)`` computes the Probability Density Function at values ``x``\n",
    "  in the case of continuous distributions\n",
    "* ``dist.rvs(N)`` computes ``N`` random variables distributed according\n",
    "  to the given distribution\n",
    "Many further options exist; refer to the documentation of ``scipy.stats``\n",
    "for more details.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "W_values = [1.0, 2.0, 3.0]\n",
    "linestyles = ['-', '--', ':']\n",
    "mu = 0\n",
    "x = np.linspace(-2, 2, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "\n",
    "for W, ls in zip(W_values, linestyles):\n",
    "    left = mu - 0.5 * W\n",
    "    dist = uniform(left, W)\n",
    "\n",
    "    plt.plot(x, dist.pdf(x), ls=ls, c='black',\n",
    "             label=r'$\\mu=%i,\\ W=%i$' % (mu, W))\n",
    "\n",
    "plt.xlim(-1.7, 1.7)\n",
    "plt.ylim(0, 1.2)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu, W)$')\n",
    "plt.title('Uniform Distribution')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement [uniform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.uniform.html#scipy.stats.uniform) in `scipy` as follows.  We'll use the methods listed at the bottom of the link to complete the cell: `dist.rvs(size=N)` which produces `N` random draws from the distribution and `dist.pdf(x)` which returns the value of the pdf at a given $x$. Lots of distributions can be accessed and used in a similar way.  \n",
    "\n",
    "Create a uniform distribution with parameters `loc=0`,  `scale=2`, and `N=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 \n",
    "distU = scipy.stats.uniform(-1,2)\n",
    "draws = distU.rvs(N) # ten random draws\n",
    "print(draws)\n",
    "\n",
    "p = distU.pdf(1) # pdf evaluated at x=1\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = distU.cdf(1) # pdf evaluated at x=1\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution\n",
    "\n",
    "As many of you know, the Gaussian distribution pdf is given by\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "It is also called the **normal distribution** and can be noted by $\\mathscr{N}(\\mu,\\sigma)$.\n",
    "\n",
    "\n",
    "We love using Gaussians in physics and astronomy because they can approximate many distributions and are also super easy to work with. **The convolution of two Gaussians results in a Gaussian.**  So $\\mathscr{N}(\\mu_1,\\sigma_1)$ convolved with $\\mathscr{N}(\\mu_2,\\sigma_2)$ is $\\mathscr{N}(\\mu_1+\\mu_2,\\sqrt{\\sigma_1^2+\\sigma_2^2})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of a Gaussian distribution\n",
    "----------------------------------\n",
    "Figure 3.8.\n",
    "This shows an example of a gaussian distribution with various parameters.\n",
    "We'll generate the distribution using::\n",
    "    dist = scipy.stats.norm(...)\n",
    "Where ... should be filled in with the desired distribution parameters\n",
    "Once we have defined the distribution parameters in this way, these\n",
    "distribution objects have many useful methods; for example:\n",
    "* ``dist.pmf(x)`` computes the Probability Mass Function at values ``x``\n",
    "  in the case of discrete distributions\n",
    "* ``dist.pdf(x)`` computes the Probability Density Function at values ``x``\n",
    "  in the case of continuous distributions\n",
    "* ``dist.rvs(N)`` computes ``N`` random variables distributed according\n",
    "  to the given distribution\n",
    "Many further options exist; refer to the documentation of ``scipy.stats``\n",
    "for more details.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distributions to be plotted\n",
    "sigma_values = [0.5, 1.0, 2.0]\n",
    "linestyles = ['-', '--', ':']\n",
    "mu = 0\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "for sigma, ls in zip(sigma_values, linestyles):\n",
    "    # create a gaussian / normal distribution\n",
    "    dist = norm(mu, sigma)\n",
    "\n",
    "    plt.plot(x, dist.pdf(x), ls=ls, c='black',\n",
    "             label=r'$\\mu=%i,\\ \\sigma=%.1f$' % (mu, sigma))\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0, 0.85)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distG = scipy.stats.norm(loc =100 , scale=15) # Normal distribution with mean = 100, stdev = 15\n",
    "draws = distG.rvs(10) # 10 random draws\n",
    "p = distG.pdf(0) # pdf evaluated at x=0\n",
    "\n",
    "print(draws)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cumulative distribution function, cdf is the integral of pdf from $x'=-\\infty$ to $x'=x$:\n",
    "\n",
    "$$\\mathrm{cdf}(x|\\mu,\\sigma) = \\int_{-\\infty}^{x'} p(x'|\\mu,\\sigma) dx',$$\n",
    "\n",
    "where $\\mathrm{cdf}(\\infty) = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's play with Gaussians! Or Normal distributions, N(mu,sigma)\n",
    "\n",
    "xgrid = np.linspace(-100,200,1000) # generate distribution for a uniform grid of x values\n",
    "gaussPDF = distG.pdf(xgrid)  # this is a function of xgrid\n",
    "\n",
    "# actual plotting\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "# Python3 f strings are awesome!\n",
    "plt.plot(xgrid, gaussPDF, ls='-', c='black', \n",
    "         label=f'$\\mu={mu},\\ \\sigma={sigma}$')\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(0, 0.03)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same as above but now with the cdf method\n",
    "gaussCDF = distG.cdf(xgrid)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "plt.plot(xgrid, gaussCDF, ls='-', c='black', \n",
    "         label=r'$\\mu=%i,\\ \\sigma=%i$' % (mu, sigma))\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(-0.01, 1.01)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$CDF(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian confidence levels\n",
    "\n",
    "The probability of a measurement drawn from a Gaussian distribution that is between $\\mu-a$ and $\\mu+b$ is\n",
    "\n",
    "$$\\int_{\\mu-a}^{\\mu+b} p(x|\\mu,\\sigma) dx.$$\n",
    "\n",
    "- For $a=b=1\\sigma$, we get the familar result of 68.3%.  \n",
    "- For $a=b=2\\sigma$ it is 95.4%.  \n",
    "- For $a=b=3\\sigma$ it is 99.7%. \n",
    "\n",
    "So we refer to the range $\\mu \\pm 1\\sigma$, $\\mu \\pm 2\\sigma$, and $\\mu \\pm 3\\sigma$ as the 68%, 95%, and 99.7% **confidence limits**, respectively. Note that if your distribution is not Gaussian, then these confidence intervals will be different!\n",
    "\n",
    "***We often still refer to uncertainty regions of distributions as $1\\sigma$ or $2\\sigma$ regions, which for non-Gaussian distributions usually means (for $1\\sigma$) the region enclosing the $16\\%$ and $84\\%$ quantiles.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = distG.cdf(100+15)\n",
    "lower = distG.cdf(100-15)\n",
    "p = upper-lower\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability enclosed between $-2\\sigma$ and $+4\\sigma$? (*Verify first that you get the correct answer for the bullet points above!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = distG.cdf(100+4*15)\n",
    "lower = distG.cdf(100-2*15)\n",
    "p = upper-lower\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Normal\n",
    "\n",
    "Note that if $x$ is Gaussian distributed with $\\mathscr{N}(\\mu,\\sigma)$, then $y=\\exp(x)$ will have a **log-normal** distribution, where the mean of y is $\\exp(\\mu + \\sigma^2/2)$, the median is $\\exp(\\mu)$, and the mode is $\\exp(\\mu-\\sigma^2)$.  Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "x = scipy.stats.norm(0,1) # mean = 0, stdev = 1\n",
    "y = np.exp(x.rvs(100))\n",
    "\n",
    "print(y.mean())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catch here is that stats.norm(0,1) returns an *object* and not something that we can just do math on in the expected manner.  What *can* you do with it?  Try ```dir(x)``` to get a list of all the methods and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distLN = scipy.stats.norm(0,1) # mean = 0, stdev = 1\n",
    "x = distLN.rvs(10000)\n",
    "y = np.exp(x)\n",
    "\n",
    "print(np.exp(0 + 0.5*1), y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\chi^2$ Distribution\n",
    "\n",
    "We'll run into the $\\chi^2$ distribution when we talk about Maximum Likelihood in the next lecture.\n",
    "\n",
    "If we have a Gaussian distribution with values ${x_i}$ and we scale and normalize them according to\n",
    "$$z_i = \\frac{x_i-\\mu}{\\sigma},$$\n",
    "then the sum of squares, $Q$ \n",
    "$$Q = \\sum_{i=1}^N z_i^2,$$\n",
    "will follow the $\\chi^2$ distribution.  The *number of degrees of freedom*, $k$ is given by the number of data points, $N$ (minus any constraints).  The pdf of $Q$ given $k$ defines $\\chi^2$ and is given by\n",
    "$$p(Q|k)\\equiv \\chi^2(Q|k) = \\frac{1}{2^{k/2}\\Gamma(k/2)}Q^{k/2-1}\\exp(-Q/2),$$\n",
    "where $Q>0$ and the $\\Gamma$ function would just be the usual factorial function if we were dealing with integers, but here we have half integers.\n",
    "\n",
    "This is ugly, but it is really just a formula like anything else.  Note that the shape of the distribution *only* depends on the sample size $N=k$ and not on $\\mu$ or $\\sigma$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "Example of a chi-squared distribution\n",
    "---------------------------------------\n",
    "Figure 3.14.\n",
    "This shows an example of a :math:`\\chi^2` distribution with various parameters.\n",
    "We'll generate the distribution using::\n",
    "    dist = scipy.stats.chi2(...)\n",
    "Where ... should be filled in with the desired distribution parameters\n",
    "Once we have defined the distribution parameters in this way, these\n",
    "distribution objects have many useful methods; for example:\n",
    "* ``dist.pmf(x)`` computes the Probability Mass Function at values ``x``\n",
    "  in the case of discrete distributions\n",
    "* ``dist.pdf(x)`` computes the Probability Density Function at values ``x``\n",
    "  in the case of continuous distributions\n",
    "* ``dist.rvs(N)`` computes ``N`` random variables distributed according\n",
    "  to the given distribution\n",
    "Many further options exist; refer to the documentation of ``scipy.stats``\n",
    "for more details.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "k_values = [1, 2, 5, 7]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "mu = 0\n",
    "x = np.linspace(-1, 20, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "fig.subplots_adjust(bottom=0.12)\n",
    "\n",
    "for k, ls in zip(k_values, linestyles):\n",
    "    dist = chi2(k, mu)\n",
    "\n",
    "    plt.plot(x, dist.pdf(x), ls=ls, c='black',\n",
    "             label=r'$k=%i$' % k)\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$Q$')\n",
    "plt.ylabel(r'$p(Q|k)$')\n",
    "plt.title(r'$\\chi^2\\ \\mathrm{Distribution}$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared per degree of freedom\n",
    "\n",
    "In practice we frequently divide $\\chi^2$ by the number of degrees of freedom, and work with:\n",
    "\n",
    "$$\\chi^2_\\mathrm{dof} = \\frac{1}{N-1} \\sum_{i=1}^N \\left(\\frac{x_i-\\overline{x}}{\\sigma}\\right)^2$$\n",
    "\n",
    "which (for large $k$) is distributed as\n",
    "\n",
    "$$ p(\\chi^2_\\mathrm{dof}) \\sim \\mathscr{N}\\left(1, \\sqrt{\\frac{2}{N-1}}\\right) $$\n",
    "\n",
    "(where $k = N-1$, and $N$ is the number of samples). Therefore, we expect $\\chi^2_\\mathrm{dof}$ to be 1, to within a few $\\sqrt{\\frac{2}{N-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution\n",
    "\n",
    "This is a distribution for a discrete variable, telling you the probability of $k$ events occuring within a certain time when the mean is $\\mu$. \n",
    "\n",
    "$$ p(k|\\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!} $$\n",
    "\n",
    "where the mean $\\mu$ completely characterizes the distribution. The mode is $(\\mu-1)$, the standard deviation is $\\sqrt{\\mu}$, the skewness is $1/\\sqrt{\\mu}$, and the kurtosis is $1/\\mu$.\n",
    "\n",
    "As $\\mu$ increases the Poisson distribution becomes more and more similar to a Gaussian with $\\mathcal{N}(\\mu,\\sqrt{\\mu})$. The Poisson distribution is sometimes called the ***law of small numbers*** or ***law of rare events***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=True)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "mu_values = [1, 5, 15]\n",
    "linestyles = ['-', '--', ':']\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "#   we generate it using scipy.stats.poisson().  Once the distribution\n",
    "#   object is created, we have many options: for example\n",
    "#   - dist.pmf(x) evaluates the probability mass function in the case of\n",
    "#     discrete distributions.\n",
    "#   - dist.pdf(x) evaluates the probability density function for\n",
    "#   evaluates\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "for mu, ls in zip(mu_values, linestyles):\n",
    "    # create a poisson distribution\n",
    "    # we could generate a random sample from this distribution using, e.g.\n",
    "    #   rand = dist.rvs(1000)\n",
    "    dist = poisson(mu)\n",
    "    x = np.arange(-1, 200)\n",
    "\n",
    "    plt.plot(x, dist.pmf(x), color='black',\n",
    "             linestyle=ls,\n",
    "             label=r'$\\mu=%i$' % mu)\n",
    "\n",
    "    plt.scatter(x, dist.pmf(x), edgecolor='black',facecolor='white')\n",
    "    \n",
    "plt.xlim(-0.5, 30)\n",
    "plt.ylim(0, 0.4)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu)$')\n",
    "plt.title('Poisson Distribution')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student's $t$ Distribution\n",
    "\n",
    "Another distribution that we'll see later is the Student's $t$ Distribution.\n",
    "\n",
    "If you have a sample of $N$ measurements, $\\{x_i\\}$, drawn from a Gaussian distribution, $\\mathscr{N}(\\mu,\\sigma)$, and you apply the transform\n",
    "\n",
    "$$t = \\frac{\\overline{x}-\\mu}{s/\\sqrt{N}},$$\n",
    "\n",
    "then $t$ will be distributed according to Student's $t$ with the following pdf (for $k$ degrees of freedom):\n",
    "\n",
    "$$p(x|k) = \\frac{\\Gamma(\\frac{k+1}{2})}{\\sqrt{\\pi k} \\Gamma(\\frac{k}{2})} \\left(1+\\frac{x^2}{k}\\right)^{-\\frac{k+1}{2}}$$\n",
    "\n",
    "As with a Gaussian, Student's $t$ is bell shaped, but has \"heavier\" tails.\n",
    "\n",
    "Note the similarity between $t$ and $z$ for a Gaussian (as defined in the $\\chi^2$ section above), which reflects the difference between data-derived estimates of the mean and standard deviation and their true values.\n",
    "\n",
    "In fact, although often approximated as a Gaussian distribution, the mean of a sample actually follows a Student's $t$ distribution (**Try this at home: check this property with scipy**). This matters when sample sizes are small, but mostly irrelevant for \"Big Data\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of Student's t distribution\n",
    "-----------------------------------\n",
    "Figure 3.15.\n",
    "This shows an example of Student's t distribution with various parameters.\n",
    "We'll generate the distribution using::\n",
    "    dist = scipy.stats.student_t(...)\n",
    "Where ... should be filled in with the desired distribution parameters\n",
    "Once we have defined the distribution parameters in this way, these\n",
    "distribution objects have many useful methods; for example:\n",
    "* ``dist.pmf(x)`` computes the Probability Mass Function at values ``x``\n",
    "  in the case of discrete distributions\n",
    "* ``dist.pdf(x)`` computes the Probability Density Function at values ``x``\n",
    "  in the case of continuous distributions\n",
    "* ``dist.rvs(N)`` computes ``N`` random variables distributed according\n",
    "  to the given distribution\n",
    "Many further options exist; refer to the documentation of ``scipy.stats``\n",
    "for more details.\n",
    "\"\"\"\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from scipy.stats import t as student_t\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# This function adjusts matplotlib settings for a uniform feel in the textbook.\n",
    "# Note that with usetex=True, fonts are rendered with LaTeX.  This may\n",
    "# result in an error if LaTeX is not installed on your system.  In that case,\n",
    "# you can set usetex to False.\n",
    "if \"setup_text_plots\" not in globals():\n",
    "    from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=8, usetex=False)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the distribution parameters to be plotted\n",
    "mu = 0\n",
    "k_values = [1E10, 2, 1, 0.5]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "for k, ls in zip(k_values, linestyles):\n",
    "    dist = student_t(k, 0)\n",
    "\n",
    "    if k >= 1E10:\n",
    "        label = r'$\\mathrm{t}(k=\\infty)$'\n",
    "    else:\n",
    "        label = r'$\\mathrm{t}(k=%.1f)$' % k\n",
    "\n",
    "    plt.plot(x, dist.pdf(x), ls=ls, c='black', label=label)\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(0.0, 0.45)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|k)$')\n",
    "plt.title(\"Student's $t$ Distribution\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the point of all these distributions?\n",
    "\n",
    "* There are many other distributions that we haven't covered here (see the textbook).\n",
    "* The point is that we are going to make some measurement. \n",
    "* To understand the significance of our measurement, we want to know how likely it is that we would get that measurement in our experiment by random chance. \n",
    "* To determine that we need to know the shape of the distribution. Let's say that we find that $x=6$. If our data is $\\chi^2$ distributed with 2 degrees of freedom, then we would integrate the $k=2$ curve above from 6 to $\\infty$ to determine how likely it is that we would have gotten 6 or larger by chance.  If our distribution was instead $t$ distributed, we would get a *very* different answer.  \n",
    "\n",
    "Note that it is important that you decide *ahead of time* what the metric will be for deciding whether this result is significant or not.  More on this later, but see [this article](http://fivethirtyeight.com/features/science-isnt-broken/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUN FACT:** \"Student\" was the pen name of W. S. Gosset, who worked for the Guinness brewery in Dublin, Ireland. He was interested in the statistical analysis of small samples, e.g., the chemical properties of barley when the sample size might be as small as $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg](https://thatsmaths.files.wordpress.com/2014/04/gosset-plaque.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to get your hands dirty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A gaussian integral\n",
    "\n",
    "Using Monte Carlo integration, check that\n",
    "\n",
    "$$ \\int_0^\\infty x^3 \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\,dx = 2\\sigma^4 .$$\n",
    "\n",
    "- Does the result converge with the number of samples? And how does the error go down?\n",
    "- Do it many times. For a given $N$, how are the result distributed? We'll talk about model fitting at lenght later on, but for now try to fit it by hand with a parametrized model. (If N is large enough you should get something that looks *very* accurate! And if $N$ is small?)\n",
    "- How does the distribution change if $N$ increases?\n",
    "\n",
    "(Hint: think about sample mean and sample variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My solution](https://github.com/dgerosa/astrostatistics_bicocca_2023/blob/main/solutions/S03_cubegaussianintegral.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kicking horses\n",
    "\n",
    "A famous early  application of low-number statistics was an analysis of Prussian cavalryman horse-kick deaths by [Bortkiewicz](https://www.wikiwand.com/en/Ladislaus_Bortkiewicz) in 1898.\n",
    "\n",
    "He studied the distribution of 122 men kicked to death by horses among 10 Prussian army corps within 20 years (so a total number of corpes of 200). He recorded the number of corps with a given number of deaths:\n",
    "\n",
    "| Number of deaths | Number of groups |\n",
    "| --- | --- |\n",
    "| 0 | 109 |\n",
    "| 1 | 65 |\n",
    "| 2 | 22 |\n",
    "| 3 | 3 |\n",
    "| 4 | 1 |\n",
    "\n",
    "- Plot the resulting probability distribution. Careful with the normalization\n",
    "- How does it look like? Again try to fit it by hand.\n",
    "\n",
    "(Hint: think about sample mean and sample variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[My solution](https://github.com/dgerosa/astrostatistics_bicocca_2023/blob/main/solutions/S04_horsekicks.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
